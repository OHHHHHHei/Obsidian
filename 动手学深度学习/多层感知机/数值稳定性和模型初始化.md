# 梯度消失和梯度爆炸

### 梯度消失与爆炸的数学分析

**核心公式回顾：反向传播的链式连乘**

> 公式 (4.8.2)：
> 
> $$\frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(l)}} = \underbrace{\frac{\partial \mathbf{h}^{(L)}}{\partial \mathbf{h}^{(L-1)}} \cdots \frac{\partial \mathbf{h}^{(l+1)}}{\partial \mathbf{h}^{(l)}}}_{\text{连乘项 } M} \cdot \frac{\partial \mathbf{h}^{(l)}}{\partial \mathbf{W}^{(l)}}$$
- **讲解**：
    - 为了算第 $l$ 层的梯度，我们需要把从输出层 $L$ 到 $l+1$ 层的所有中间层的导数矩阵 $\mathbf{M}$ 乘起来。
### 梯度消失

![[Pasted image 20251129125439.png]]

**Sigmoid 激活函数**
- Sigmoid 函数：$\sigma(x) = \frac{1}{1+e^{-x}}$
- 导数：$\sigma'(x) = \sigma(x)(1-\sigma(x))$
- **问题分析**：
    - 导数最大值在 $x=0$ 处，也只有 $0.25$。
    - 当输入很大或很小时，导数趋近于 $0$。
    - 如果你有 10 层网络，全用 Sigmoid，梯度的系数最大也就是 $0.25^{10} \approx 0.00000095$。
- **后果**：靠近输入的层（底层）几乎收不到梯度信号，导致底层参数不更新，网络无法学习深层特征。
- **对策**：现在大家普遍使用 **ReLU** ($\max(0, x)$)，因为它的导数在正区间恒为 1，不会衰减。
### 梯度爆炸
- 假设没有激活函数，每层都是线性变换 $\mathbf{W}$。
- 如果 $\mathbf{W}$ 的元素稍微大一点（比如特征值大于 1），连乘 100 次后，数值会变成天文数字（如 $1.5^{100} \approx 4 \times 10^{17}$）。
- 文档中的代码演示了这一点：初始化矩阵元素服从 $N(0, 1)$，连乘 100 次后，矩阵元素变得巨大，无法进行有效的梯度下降。
- **后果**：参数更新步长过大，Loss 震荡甚至变成 NaN。
### 打破对称性
- 如果我们把所有权重都初始化为**同一个常数**（比如全 0 或全 1）。
- 在前向传播时，同一层的每个神经元接收到的输入是一样的，输出也是一样的。
- 在反向传播时，它们算出来的梯度也是一样的。
- 更新后，它们依然一样。
- **后果**：无论网络多宽，实际上等效于每层只有一个神经元。这被称为**对称性**。
- **对策**：必须进行**随机初始化**，打破这种对称性，让每个神经元有机会学习不同的特征。

# 参数初始化

## 默认初始化
如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。
## Xavier初始化

**设定背景**：
- 关注某一层全连接层：$o_i = \sum_{j=1}^{n_{in}} w_{ij} x_j$。
- $n_{in}$：这一层的输入个数（上一层神经元数）。
- $w_{ij}$：权重，假设是从均值为0、方差为 $\sigma^2$ 的分布中随机采样的。
- $x_j$：输入，假设也是均值为0、方差为 $\gamma^2$ 的分布。

步骤 1：前向传播的方差分析
我们想看看输出 $o_i$ 的方差和输入 $x_j$ 的方差有什么关系。
> **公式推导**：
> 1. 均值 $E[o_i]$：
>     由于 $w$ 和 $x$ 均值为0且独立，$E[w x] = E[w]E[x] = 0$。
>     所以 $\sum$ 求和后均值依然为 0。
> 2. 方差 $Var[o_i]$：
>     根据方差性质，对于独立变量的和，方差等于各变量方差之和：
>     $Var[o_i] = \sum_{j=1}^{n_{in}} Var[w_{ij} x_j]$
>     对于独立零均值变量，乘积的方差等于方差的乘积：$$Var[w_{ij} x_j] = E[(w_{ij} x_j)^2] - (E[w_{ij} x_j])^2 = E[w^2]E[x^2] - 0 = Var[w]Var[x] = \sigma^2 \gamma^2$$
>     因为有 $n_{in}$ 个这样的项相加，所以：
>     $Var[o_i] = n_{in} \sigma^2 \gamma^2$

分析：
如果我们要保持数值稳定，即输入信号的强度经过一层后不被放大也不被缩小，我们需要输出方差等于输入方差，即 $Var[o_i] = \gamma^2$。
代入上面的公式，得到条件 A：
$$n_{in} \sigma^2 = 1$$

步骤 2：反向传播的方差分析
同理，反向传播时，梯度也是经过转置权重的乘法。为了防止梯度消失或爆炸，我们需要梯度的方差保持不变。
经过类似的推导，我们得到条件 B：
$$n_{out} \sigma^2 = 1$$

**步骤 3：Xavier 的折中方案（Harmonic Mean）**
- **矛盾**：除非 $n_{in} = n_{out}$，否则我们无法同时满足 $n_{in} \sigma^2 = 1$ 和 $n_{out} \sigma^2 = 1$。
- 解决方案：取折中，让 $n_{in}$ 和 $n_{out}$ 的均值与 $\sigma^2$ 的乘积为 1。
    
    即：$\frac{1}{2}(n_{in} + n_{out}) \sigma^2 = 1$。
    
由此解出目标方差：
$$\sigma^2 = \frac{2}{n_{in} + n_{out}}$$
**步骤 4：具体的分布公式**
有了目标方差 $\sigma^2 = \frac{2}{n_{in} + n_{out}}$，我们可以设计具体的初始化分布：
1. 正态分布（Normal Distribution）：
    直接让 $N(0, \sigma^2)$ 中的 $\sigma = \sqrt{\frac{2}{n_{in} + n_{out}}}$。
2. **均匀分布（Uniform Distribution）**：
    - 假设从区间 $[-a, a]$ 采样。
    - 均匀分布的方差公式是 $\frac{(2a)^2}{12} = \frac{a^2}{3}$。
    - 令 $\frac{a^2}{3} = \sigma^2 = \frac{2}{n_{in} + n_{out}}$，解出 $a$。
    - 得到边界 $a$：$$U\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)$$
