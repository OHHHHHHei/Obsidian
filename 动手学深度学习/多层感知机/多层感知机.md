# 隐藏层

- **问题提出**：Softmax回归，它是**线性**的（仿射变换）。但现实世界很多问题（如分类猫狗、信用评分）不是单调线性的。
- **解决方案**：为了模拟更复杂的函数关系，我们在输入和输出之间加入**隐藏层 (Hidden Layer)**，形成**多层感知机 (MLP)**。
- **数学陷阱（重点）**：简单的堆叠全连接层（如果不加非线性变换），数学上等价于**单层**线性模型。

**在网络中加入隐藏层**
- **定义**：为了解决上述问题，我们在输入层和输出层之间加入一层神经元，称为**隐藏层**。
- **架构图解**：
    - 输入层 $\rightarrow$ 隐藏层 $\rightarrow$ 输出层。
    - 这种架构被称为 **多层感知机 (MLP)**。
    - **全连接**：每一层的每个节点都与下一层的所有节点相连。
![[Pasted image 20251126232314.png]]
# 从线性到非线性

- - $X \in \mathbb{R}^{n \times d}$：输入矩阵（小批量），$n$是样本数，$d$是特征维度。
    - $H \in \mathbb{R}^{n \times h}$：隐藏层输出（Hidden），$h$是隐藏单元个数。
    - $O \in \mathbb{R}^{n \times q}$：最终输出，$q$是输出类别数。
    - $W^{(1)}, b^{(1)}$：第一层（隐藏层）的权重和偏置。
    - $W^{(2)}, b^{(2)}$：第二层（输出层）的权重和偏置。
- 如果模型是线性的（没有激活函数）：
    我们按步骤计算：
    1. 计算隐藏层：$H = X W^{(1)} + b^{(1)}$
    2. 计算输出层：$O = H W^{(2)} + b^{(2)}$
    推导（代入消元）：
    将第1步代入第2步：$$O = (X W^{(1)} + b^{(1)}) W^{(2)} + b^{(2)}$$
    利用矩阵乘法的分配律展开：$$O = X (W^{(1)} W^{(2)}) + (b^{(1)} W^{(2)} + b^{(2)})$$
**观察结果**：
- $W^{(1)} W^{(2)}$ 只是两个矩阵相乘，结果还是一个矩阵，我们记为 $W_{new}$。
- $b^{(1)} W^{(2)} + b^{(2)}$ 只是向量的线性组合，结果还是一个向量，记为 $b_{new}$。
- 于是公式变成了：$O = X W_{new} + b_{new}$。
结论：
这依然是一个单层的线性模型！ 无论你堆叠了多少层，只要没有非线性环节，它们最终都可以被“压缩”成一个矩阵乘法。

## 引入激活函数
- 为了让网络能逼近复杂函数（比如体温的波峰，或者猫的形状），我们需要在生成 $H$ 之后，加一个非线性的函数 $\sigma$（Sigma）。$$H = \sigma(X W^{(1)} + b^{(1)})$$$$O = H W^{(2)} + b^{(2)}$$
    这里的 $\sigma$ 是按元素运算的。加上这个 $\sigma$ 后，上面的合并推导就不成立了，模型就拥有了非线性表达能力。
# 通用近似定理
- 理论上，只要隐藏层节点足够多，并配合非线性激活函数，单隐藏层的MLP可以拟合**任意**连续函数。这证明了神经网络的强大潜力。
# 激活函数

## ReLU函数
$$ReLU(x) = \max(x, 0)$$
![[Pasted image 20251126234849.png]]
- **物理图像**：
    - 当 $x \le 0$ 时，输出为 0（死区）。
    - 当 $x > 0$ 时，输出为 $x$（线性通过）。
    - 图像看起来像一个折线，但在 $x=0$ 处有一个转折点。
- 导数（梯度）：
    这是最关键的特性。
    - 当 $x > 0$ 时，导数为 1。
    - 当 $x < 0$ 时，导数为 0。
    - 当 $x = 0$ 时，数学上不可导，但在工程实现中通常人为规定导数为 0 或 1。
- **为什么它最受欢迎？**
    1. **计算极其简单**：只需要判断是否大于0，没有任何指数运算（$e^x$），这对于跑几十亿次计算的GPU来说非常省力。
    2. **缓解梯度消失**：在 $x > 0$ 的区域，导数恒为 1。这意味着在反向传播时，梯度可以无损地传回前面的层，不会像 Sigmoid 那样因为乘以很小的小数而迅速衰减为 0。
    3. **自动化类比**：这就像控制系统中的 **“死区” (Dead Zone)** 环节，只响应正向的大信号，过滤掉负向信号。
- **变体 pReLU (Parameterized ReLU)**：
    为了防止神经元“彻底死掉”（即 $x < 0$ 时导数永远为 0，无法更新），pReLU 给负半区加了一个很小的斜率 $\alpha$：$$pReLU(x) = \max(0, x) + \alpha \min(0, x)$$ 这样即使输入是负的，也能有微弱的梯度回传。

## Sigmoid函数

sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
$$sigmoid(x) = \frac{1}{1 + e^{-x}}$$
![[Pasted image 20251126235123.png]]
- **物理图像**：
    - S形曲线。
    - 将任意实数输入 $(-\infty, \infty)$ 压缩（Squashing）到区间 $(0, 1)$ 之间。
    - 当输入非常大时，输出趋近 1；输入非常小时，输出趋近 0；输入为 0 时，输出为 0.5。

### 导数推导

Sigmoid 的导数可以用它自己来表示，非常优雅：

$$\frac{d}{dx}sigmoid(x) = \frac{e^{-x}}{(1+e^{-x})^2} = sigmoid(x)(1 - sigmoid(x))$$
![[Pasted image 20251126235310.png]] 
- **观察导数图像**：当 $x=0$ 时，导数最大，仅为 0.25。当 $x$ 很大或很小时，导数趋近于 0。
- **致命弱点（梯度消失）**：在深层网络中，梯度是通过连乘法则反向传播的。如果每一层的梯度都小于 0.25，乘个十几层，梯度就变成了 $0.25^{10} \approx 0.0000009$，基本上就是 0 了。参数无法更新，网络就学不动了。
- **应用场景**：现在主要用于**输出层**做二分类（输出概率），很少用在隐藏层了。
## Tanh函数
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。
$$tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}$$
![[Pasted image 20251126235507.png]]
- **特性**
	- 形状和 Sigmoid 很像，但它的值域是 $(-1, 1)$。
	- 它是**关于原点对称**的。这意味着它的输出均值更接近 0。
- **优点**：在神经网络训练中，以 0 为中心的数据通常收敛得更快，所以 Tanh 在隐藏层中通常比 Sigmoid 表现好。
### 导数：
$$\frac{d}{dx}tanh(x) = 1 - tanh^2(x)$$

- 当 $x=0$ 时，导数为 1（比 Sigmoid 的 0.25 大，缓解了梯度消失，但没完全解决） 
- 当 $|x|$ 很大时，导数依然趋近于 0（饱和区）。
