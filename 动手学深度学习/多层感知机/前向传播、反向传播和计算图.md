# 前向传播

我们要推导一个**单隐藏层**的多层感知机。
- **输入**：$\mathbf{x} \in \mathbb{R}^d$（这是一个 $d$ 维的列向量，代表一个样本）。
- **简化**：为了推导清爽，我们在隐藏层中**忽略了偏置项**（bias），只考虑权重。

**步骤 1：从输入到隐藏层（线性变换）**
> **公式 (4.7.1)**：$\mathbf{z} = \mathbf{W}^{(1)}\mathbf{x}$
- **讲解**：
    - 这是第一层的线性计算。
    - $\mathbf{W}^{(1)}$ 是隐藏层的权重矩阵，维度是 $h \times d$（$h$是隐藏单元数，$d$是输入特征数）。
    - $\mathbf{x}$ 是 $d \times 1$ 的向量。
    - 相乘后，$\mathbf{z}$ 是一个 $h \times 1$ 的向量。  

**步骤 2：隐藏层的激活（非线性变换）**

> **公式 (4.7.2)**：$\mathbf{h} = \phi(\mathbf{z})$
- **讲解**：
    - $\phi$ 是激活函数（如 ReLU, Sigmoid）。
    - **关键点**：这个操作是**按元素**（element-wise）进行的。向量 $\mathbf{z}$ 里的每一个元素都经过 $\phi$ 变成 $\mathbf{h}$ 里的对应元素。
    - $\mathbf{h}$ 的长度依然是 $h$。
    - **为什么这样做？** 引入非线性，否则多少层网络叠加都等价于一层线性变换，无法拟合复杂函数。

**步骤 3：从隐藏层到输出层**

> **公式 (4.7.3)**：$\mathbf{o} = \mathbf{W}^{(2)}\mathbf{h}$
- **讲解**：
    - 这是输出层的计算。
    - $\mathbf{W}^{(2)}$ 是输出层权重，维度是 $q \times h$（$q$是输出类别数）。
    - $\mathbf{o}$ 是输出向量，长度为 $q$。

**步骤 4：计算预测损失**

> **公式 (4.7.4)**：$L = l(\mathbf{o}, y)$
- **讲解**
    - $l$ 是损失函数（如交叉熵损失）。
    - $y$ 是真实标签。
    - $L$ 是一个**标量**（Scalar），代表这个样本预测得有多差。

**步骤 5：计算正则化项（$L_2$ Regularization）**

> **公式 (4.7.5)**：$s = \frac{\lambda}{2} \left( \|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2 \right)$
- **讲解**：这是为了防止过拟合加入的惩罚项。
    - $\lambda$：超参数，控制正则化强度。
    - $\|\cdot\|_F$：**弗罗贝尼乌斯范数（Frobenius norm）**。这听起来很高大上，其实就是把矩阵里**所有元素的平方和开根号**。
    - $\|\mathbf{W}\|_F^2$：有了平方，根号抵消了，实际上就是**矩阵中所有权重的平方和**。
    - **为什么有个 $\frac{1}{2}$？** 这是数学上的“小心机”。因为 $x^2$ 的导数是 $2x$，前面的 $\frac{1}{2}$ 刚好能和导数出来的 $2$ 抵消，让后面的梯度公式更干净（没有系数 2）。

**步骤 6：总目标函数**

> **公式 (4.7.6)**：$J = L + s$
- **讲解**：
    - $J$ 称为**目标函数（Objective Function）**。
    - 我们训练神经网络，最终优化的就是这个 $J$，既要让预测准（$L$ 小），又要让模型简单（$s$ 小）。

# 前向传播计算图

![[Pasted image 20251129122918.png]]
# 反向传播

目标函数 $J$ 对参数 $\mathbf{W}^{(1)}$ 和 $\mathbf{W}^{(2)}$ 的梯度（即 $\frac{\partial J}{\partial \mathbf{W}^{(1)}}$ 和 $\frac{\partial J}{\partial \mathbf{W}^{(2)}}$）。为了得到它们，我们需要从最后面往回推。

**步骤 0：起步 - 目标函数的分解**
> **公式 (4.7.6)**：$J = L + s$
- 我们先算最外层的梯度。显然，$\frac{\partial J}{\partial L} = 1$，$\frac{\partial J}{\partial s} = 1$。
- 这意味着总梯度的来源有两部分：一部分来自预测误差 $L$，一部分来自正则化项 $s$。

**步骤 1：输出层的梯度（预测误差回传）**
> **公式 (4.7.9)**：$\frac{\partial J}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{o}}$
- **讲解**：这是反向传播的**起点**。$\frac{\partial L}{\partial \mathbf{o}}$ 取决于你具体用了什么损失函数（比如均方误差或交叉熵）。我们假设已经算出来了，记为一个已知向量。

**步骤 2：正则化项的梯度**
> **公式 (4.7.10)**：$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}$，$\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}$
- **讲解**：
    - 正则化项 $s$ 是权重的平方和乘以系数 $\lambda/2$。
    - 对平方求导就是 $2 \times (\lambda/2) \times \mathbf{W} = \lambda \mathbf{W}$。
    - 这部分梯度**直接加到**最终结果里，因为它不依赖数据流，只依赖参数本身。

**步骤 3：关于第二层权重 $\mathbf{W}^{(2)}$ 的梯度（关键！）**
> **公式 (4.7.11)**：$\frac{\partial J}{\partial \mathbf{W}^{(2)}} = \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}$
- **推导细节**：
    - 回顾前向公式：$\mathbf{o} = \mathbf{W}^{(2)}\mathbf{h}$。
    - 根据矩阵求导规则，$\mathbf{o}$ 对 $\mathbf{W}^{(2)}$ 的导数涉及到输入 $\mathbf{h}$。
    - **为什么是 $\mathbf{h}^\top$（转置）？**
        - 梯度矩阵的形状必须和 $\mathbf{W}^{(2)}$ 一样，即 $q \times h$。
        - $\frac{\partial J}{\partial \mathbf{o}}$ 是 $q \times 1$ 的列向量。
        - $\mathbf{h}$ 是 $h \times 1$ 的列向量。
        - 要得到 $q \times h$ 的矩阵，只能是 $(q \times 1) \times (1 \times h)$，即 $\frac{\partial J}{\partial \mathbf{o}} \times \mathbf{h}^\top$。
    - **物理意义**：误差信号 $\frac{\partial J}{\partial \mathbf{o}}$ 乘以 激活值 $\mathbf{h}$，决定了权重该怎么调。再加上正则化项 $\lambda \mathbf{W}^{(2)}$。

**步骤 4：误差传回隐藏层 $\mathbf{h}$**
> **公式 (4.7.12)**：$\frac{\partial J}{\partial \mathbf{h}} = {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}$
- **推导细节**：
    - 我们要继续往回传，这就需要让误差通过矩阵乘法 $\mathbf{W}^{(2)}$。
    - **为什么又是转置？**
        - 前向传播是 $\mathbf{o} = \mathbf{W}^{(2)}\mathbf{h}$ （$\mathbf{W}$ 乘 $\mathbf{h}$）。
        - 反向传播是把误差从 $\mathbf{o}$ 传回 $\mathbf{h}$，相当于逆着矩阵走，数学上就对应乘以矩阵的转置 $\mathbf{W}^\top$。
        - 维度检查：$\mathbf{W}^\top$ 是 $h \times q$，误差 $\frac{\partial J}{\partial \mathbf{o}}$ 是 $q \times 1$。相乘得到 $h \times 1$，正是 $\mathbf{h}$ 的梯度维度。

**步骤 5：穿过激活函数（到达 $\mathbf{z}$）**
> **公式 (4.7.13)**：$\frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial \mathbf{h}} \odot \phi'(\mathbf{z})$
- **讲解**：
    - $\mathbf{h} = \phi(\mathbf{z})$。
    - 因为激活函数是**按元素**操作的，互不影响，所以不需要矩阵乘法，而是**按元素乘法**（Hadamard Product，符号 $\odot$）。
    - 这意味着：上游传来的梯度 $\frac{\partial J}{\partial \mathbf{h}}$，乘以当前点激活函数的导数值 $\phi'(\mathbf{z})$。
    - **重要**：如果激活函数导数 $\phi'$ 很小（比如 Sigmoid 在两端），这里乘完梯度就会变得极小，这就是**梯度消失**的数学根源。

**步骤 6：关于第一层权重 $\mathbf{W}^{(1)}$ 的梯度**
> **公式 (4.7.14)**：$\frac{\partial J}{\partial \mathbf{W}^{(1)}} = \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}$
- **讲解**：
    - 逻辑和步骤 3 完全一样。
    - 前向是 $\mathbf{z} = \mathbf{W}^{(1)}\mathbf{x}$。
    - 反向求梯度就是 误差 $\frac{\partial J}{\partial \mathbf{z}}$ 乘以 输入 $\mathbf{x}^\top$。
    - 别忘了加上正则化项 $\lambda \mathbf{W}^{(1)}$。
# 训练神经网络

在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。