# 重新审视过拟合

- **线性模型的局限性：** 就像我们用一条直线去拟合复杂的曲线。如果特征多但样本少，它会过拟合；但如果样本足够，线性模型往往**泛化性很好**，因为它太简单了（方差低），但缺点是**灵活性差**（偏差高），无法捕捉特征之间的交互关系（比如：“只有当A和B同时发生时，结果才是C”）。
    
- **深度神经网络的特性：** 它们处于**偏差-方差谱**的另一端。它们极其灵活，能捕捉复杂的交互。
    
    - **经典案例：** 文中提到了2017年的一项研究，即使给图像打上**随机的标签**（没有任何逻辑），神经网络也能通过SGD（随机梯度下降）达到100%的训练准确率。这意味着网络只是单纯地“死记硬背”了每一张图。如果在测试集上测试，准确率会非常低（例如10分类只有10%的准确率）。这就是**严重的过拟合**。
# 扰动的稳健性

既然网络太灵活容易死记硬背，我们怎么通过数学手段限制它？
- **简单性的定义：**
    1. **参数少/值小：** 这是上一节$L_2$正则化（权重衰减）的思路。
    2. **平滑性（Smoothness）：** 这是本节的重点。即：**函数不应该对输入的微小变化敏感**。
        - **例子：** 比如你有一张猫的图片，我在图片上撒几个噪点，它应该还是猫。如果模型因为几个噪点就把它识别成狗，说明模型不稳健。
- **理论依据（Bishop, 1995）：** 在输入中加入噪声进行训练，在数学上等价于加了Tikhonov正则化（一种$L_2$正则化）。这奠定了“注入噪声”作为正则化手段的理论基础。
- **暂退法（Dropout）的诞生（Srivastava et al., 2014）：**
    - 作者将这个思路从“输入层”推广到了“内部隐藏层”。
    - **类比：** 文中提到了“有性繁殖”。如果一个基因必须依赖另一个特定基因才能工作，那这个物种很容易灭绝。Dropout强迫神经元即使在其他伙伴随机“罢工”的情况下，也要努力工作，这破坏了神经元之间的“共适应性”（co-adaptation）。
## 数学推导：如何无偏地注入噪声？

这是本节最核心的数学部分。我们要解决的问题是：**如何在注入噪声的同时，不改变层输出的统计期望？**
假设我们有一个隐藏层的中间活性值 $h$（就是神经元的输出）。我们引入暂退概率 $p$（即这个神经元被丢弃/置零的概率）。
### **标准暂退法（Inverted Dropout）的计算过程：**

我们要构造一个新的随机变量 $h'$ 来替代 $h$。
$$h' = \begin{cases} 0 & \text{概率为 } p \\ \frac{h}{1-p} & \text{其他情况 (概率为 } 1-p \text{)} \end{cases}$$
**为什么要做 $\frac{h}{1-p}$ 这个除法？：**
我们要计算 $h'$ 的**数学期望** $E[h']$。根据概率论的期望公式 $E[X] = \sum x_i P(x_i)$：
$$\begin{aligned} E[h'] &= 0 \times p + \frac{h}{1-p} \times (1-p) \\ &= 0 + h \\ &= h \end{aligned}$$

结论： $E[h'] = h$。

意义： 通过这种缩放，我们在训练时虽然随机丢弃了神经元，但该层输出的平均值（期望）保持不变。这样在测试阶段（我们不使用Dropout，保留所有神经元）时，我们就不需要对权重做任何缩放调整了，保证了训练和测试的一致性。
# 实践中的暂退法

#### 实践中的暂退法（4.6.3）
让我们回到多层感知机（MLP）的结构图。
- **图示解析（图4.6.1）：**
    - 课件中展示了一个带有1个隐藏层（5个单元）的MLP。
    - **应用Dropout后：** 假设我们以概率 $p$ 随机丢弃神经元。在图中，隐藏单元 $h_2$ 和 $h_5$ 被删除了（置零）。
    - **后果：** 此时，输出层的计算就不再依赖 $h_2$ 和 $h_5$。在反向传播时，这两个神经元的梯度也会消失（因为它们没参与计算，也就没“背锅”）。
    - **目的：** 这样强迫网络去利用剩余的神经元（$h_1, h_3, h_4$）来完成任务，防止某些神经元“偷懒”或形成过度依赖。
- **测试时的行为：**
    - 当我们训练好模型去预测新数据（测试阶段）时，我们**不丢弃**任何节点。
    - **为什么？** 我们希望利用模型学到的所有能力来进行最准确的预测。
    - **例外：** 有些研究会在测试时也开启Dropout，用来估计模型预测的“不确定性”（如果多次Dropout预测结果差异很大，说明模型对这个样本很不确定）。
## 2. 从零开始实现（4.6.4）

我们要手写一个函数 `dropout_layer` 来实现上述逻辑。
**核心算法步骤：**
1. **采样：** 从均匀分布 $U[0, 1]$ 中抽取样本，生成一个和输入 `X` 形状一样的随机矩阵。
2. **掩码（Mask）生成：**
    - 如果我们设定丢弃概率为 `dropout` (例如 0.5)。
    - 我们将随机矩阵中**大于** `dropout` 的位置设为 1（保留），小于的设为 0（丢弃）。
    - _注：不同框架API可能逻辑微调，但核心都是按概率生成0/1掩码。_
3. **应用与缩放：**
    - 计算公式：`mask * X / (1.0 - dropout)`。
    - 这就完成了“置零”和“期望值校正”两步。
**代码逻辑详解（通用逻辑）：**
```Python
def dropout_layer(X, dropout):
    # 断言：概率必须在0到1之间
    assert 0 <= dropout <= 1
    
    # 特殊情况1：全丢弃
    if dropout == 1:
        return zeros_like(X)
    
    # 特殊情况2：全保留（不干预）
    if dropout == 0:
        return X
    
    # 生成掩码：例如 dropout=0.5
    # 生成随机数 > 0.5 的概率是 0.5 (即保留概率 1-p)
    # mask 里的元素是 True (1) 或 False (0)
    mask = (random_uniform(X.shape) > dropout).float()
    
    # 核心公式：应用掩码并缩放
    return mask * X / (1.0 - dropout)
```

测试案例：
课件中给了一个例子，输入 X 是 0 到 15 的矩阵。
- 当 `dropout=0`：输出原封不动。
- 当 `dropout=0.5`：大约一半元素变0，剩下的一半元素数值翻倍（除以0.5等于乘以2）。
- 当 `dropout=1`：全变0。
## 3. 定义模型参数与模型结构

我们将Dropout应用到一个具体的模型中：Fashion-MNIST分类。
- **模型参数：**
    - 输入：784（28x28像素拉直）。
    - 输出：10（10个类别）。
    - 隐藏层：**两个**隐藏层，每层 **256** 个单元。
- **模型定义（Net函数）：**
    
    - **位置策略：** 通常在靠近输入层的地方设置较低的丢弃概率。这里第一层设为 **0.2**，第二层设为 **0.5**。
    - **正向传播逻辑（Forward）：**
        1. 计算第一层隐藏层 `H1 = ReLU(W1*X + b1)`。
        2. **判断：** `if is_training()` (如果是训练模式) -> `H1 = dropout_layer(H1, 0.2)`。
        3. 计算第二层隐藏层 `H2 = ReLU(W2*H1 + b2)`。
        4. **判断：** `if is_training()` -> `H2 = dropout_layer(H2, 0.5)`。
        5. 输出层 `Out = W3*H2 + b3` (输出层通常不加Dropout)。