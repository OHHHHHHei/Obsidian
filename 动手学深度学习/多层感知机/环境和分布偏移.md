# 分布偏移的类型
我们假设：
- $p$：**训练集**（源分布，Source）的分布。
- $q$：**测试集**（目标分布，Target）的分布。
- $\mathbf{x}$：输入特征（Covariates）。
- $y$：输出标签（Labels）。
### **类型一：协变量偏移 (Covariate Shift)**
这是最常见的一类偏移。
- **数学定义**：
    1. **输入分布变了**：$q(\mathbf{x}) \neq p(\mathbf{x})$
    2. **映射关系没变**：$q(y|\mathbf{x}) = p(y|\mathbf{x})$
- **详细讲解**：
    - “协变量”其实就是指输入特征 $\mathbf{x}$。
    - **场景**：假设我们要训练一个“猫狗分类器”。
        - **训练集 ($p$)**：全是**卡通画**的猫和狗。
        - **测试集 ($q$)**：全是**真实照片**的猫和狗。
    - **为什么叫映射没变？** 在卡通画里是猫的特征（尖耳朵、胡须），在照片里依然是猫。也就是说，如果给你一个具体的图像 $\mathbf{x}$，它属于猫的概率 $P(y|\mathbf{x})$ 是客观固定的。
        
    - **问题所在**：虽然“猫”的定义没变，但是模型在训练时只见过“线条画”的特征，没见过“真实毛发”的特征（即输入的分布 $p(\mathbf{x})$ 和 $q(\mathbf{x})$ 差异巨大），导致模型在测试集上“懵了”。

### **类型二：标签偏移 (Label Shift)**

这种情况通常出现在“因果关系”反转的场景中。
- **数学定义**：
    1. **标签分布变了**：$q(y) \neq p(y)$
    2. **反向映射没变**：$q(\mathbf{x}|y) = p(\mathbf{x}|y)$
- **详细讲解**：
    - **场景**：**医疗诊断**。
        - **训练集 ($p$)**：平时收集的数据，健康人多，流感患者少（$P(y=\text{健康}) \gg P(y=\text{流感})$）。
        - **测试集 ($q$)**：流感爆发季，患者激增（$P(y=\text{健康}) < P(y=\text{流感})$）。
    - **为什么叫反向映射没变？** $P(\mathbf{x}|y)$ 代表“如果一个人得了流感 ($y$)，他的症状 $\mathbf{x}$（发烧、咳嗽）是什么样的”。无论是不是流感季，得了流感就会发烧，这个生理规律是不变的。
        
    - **问题所在**：模型在训练时倾向于预测“健康”（因为先验概率高），到了流感季，它可能还是倾向于预测健康，从而漏诊大量病人。
#### .为什么这会导致模型失效？
用贝叶斯公式一看就明白了：
$$P(y|x) = \frac{P(x|y)P(y)}{P(x)}$$

- 我们预测时依靠的是 $P(y|x)$。
- 虽然 $P(x|y)$（症状的表现）没变。
- 但是 **$P(y)$（患病率/先验概率）变了**！
- **结论**：因为 $P(y)$ 变了，所以最终的后验概率 $P(y|x)$ 也跟着变了。模型用旧的 $P(y)$ 去算新的 $P(y|x)$，自然算不准。
### **类型三：概念偏移 (Concept Shift)**

这是最麻烦的一种，因为“真理”变了。
- **数学定义**：
    1. **映射关系变了**：$q(y|\mathbf{x}) \neq p(y|\mathbf{x})$
- **详细讲解**：
    - **场景**：**语言演变**或**地理差异**。
    - **例子**：
        - 在美国不同地区，“Pop”这个词（$\mathbf{x}$），有的地方指“汽水”，有的地方指“父亲”。
        - 如果你的模型是根据文本预测含义，同一个 $\mathbf{x}$ 在不同地区（测试集）对应的 $y$ 居然变了！
    - **时间维度**：现在的时尚潮流（$\mathbf{x}$）被认为是“酷”（$y$），十年后同样的衣服可能被认为是“土”。定义的标准随时间漂移了。

# 分布偏移纠正

我们以最常见的**协变量偏移**为例，推导如何修正训练过程。
## **(1) 经验风险最小化 (Empirical Risk Minimization)**
通常，我们训练模型是为了最小化在测试集（目标分布 $q$） 上的平均损失：
$$\text{loss} = E_{\mathbf{x} \sim q} [l(f(\mathbf{x}), y)] = \int l(f(\mathbf{x}), y) q(\mathbf{x}) d\mathbf{x}$$

但是，我们手里只有**训练集（源分布 $p$）** 的数据。如果直接用训练集算平均值，算出来的是 $\int l(...) p(\mathbf{x}) d\mathbf{x}$，这和目标不对等。
## **(2) 重要性采样 (Importance Sampling)**
为了用 $p$ 的数据算出 $q$ 的期望，我们引入一个数学恒等变换（乘以 1 再除以 1）：
$$\int l(f(\mathbf{x}), y) q(\mathbf{x}) d\mathbf{x} = \int l(f(\mathbf{x}), y) \frac{q(\mathbf{x})}{p(\mathbf{x})} p(\mathbf{x}) d\mathbf{x} = E_{\mathbf{x} \sim p} \left[ \frac{q(\mathbf{x})}{p(\mathbf{x})} l(f(\mathbf{x}), y) \right]$$

- 结论：我们依然可以在训练集 $p$ 上进行训练，只需要给每个样本 $(x_i, y_i)$ 乘上一个权重 $\beta_i$：$$\beta_i = \frac{q(\mathbf{x}_i)}{p(\mathbf{x}_i)}$$
- **直观理解**：如果一个样本 $\mathbf{x}$ 在测试集 ($q$) 出现的概率是在训练集 ($p$) 的 10 倍，那么在训练时，我们就把这个样本的 Loss 放大 10 倍，强迫模型重视它。
## **(3) 难点：如何计算权重 $\beta$？（逻辑回归技巧）**

我们不知道真实的概率密度 $p(\mathbf{x})$ 和 $q(\mathbf{x})$，怎么算比值呢？这里有一个非常巧妙的工程技巧：**训练一个二分类器**。
1. **构造数据集**：
    - 把训练集的数据标记为 $z=-1$。
    - 把测试集的数据（无标签）标记为 $z=1$。
    - 把它们混合在一起。
2. **训练分类器**：
    - 训练一个逻辑回归模型（或任何分类器），尝试区分一个数据 $\mathbf{x}$ 到底来自训练集还是测试集。
    - 如果分类器很难区分（概率接近 0.5），说明两个分布差不多，权重接近 1。
    - 如果分类器一眼就能看出 $\mathbf{x}$ 来自测试集（$P(z=1|\mathbf{x})$ 很高），说明这个 $\mathbf{x}$ 在测试集中很常见，但在训练集中很少见，权重 $\beta$ 就会很大。
3. 计算公式：
    利用贝叶斯公式，权重 $\beta(\mathbf{x})$ 可以直接由分类器的输出概率推导出来：
    $$\beta(\mathbf{x}) = \frac{P(z=1|\mathbf{x})}{P(z=-1|\mathbf{x})} \times \frac{P(z=-1)}{P(z=1)}$$
    这样我们就得到了每个训练样本的权重，代入 Loss 函数重新训练即可。

## 3. 学习讲解：标签偏移的修正

对于**标签偏移**（$P(y)$ 变了，$P(\mathbf{x}|y)$ 没变），修正逻辑类似，但更简单。

- **权重计算**：$\beta_i = \frac{q(y_i)}{p(y_i)}$。
- **源分布 $p(y)$**：统计训练集里各类标签的比例（很简单）。
- **目标分布 $q(y)$**：测试集没有标签，怎么知道比例？
    - **方法**：利用“混淆矩阵”修正。我们先用旧模型在测试集上跑一遍预测，虽然预测不准，但预测结果的分布 $\hat{q}(y)$ 包含了真实分布的信息。通过求解一个线性方程组（涉及混淆矩阵的逆），可以估计出真实的测试集标签分布 $q(y)$。
## 4. 学习形式的分类（了解即可）

为了应对环境变化，机器学习衍生出了不同的训练范式
1. **批量学习 (Batch Learning)**：
    - 一次性拿到所有数据，$f(\mathbf{x})$ 训练好后固定不变。如果不修正分布偏移，很容易失效。
2. **在线学习 (Online Learning)**：
    - 数据一个接一个来（$\mathbf{x}_t, y_t$），模型每次预测后立刻得到反馈并更新参数。
    - **优势**：天然适应**概念偏移**（Concept Shift），因为模型一直在变，紧跟最新潮流。
3. **老虎机 (Bandits)**：
    - 这是简化版的强化学习。模型不仅预测，还要做动作（Arm selection），环境会给回报。问题在于只能得到“你选的那个动作”的回报，没选的不知道。
4. **控制 (Control) / 强化学习**：
    - 环境不仅会变，而且**会因为你的行动而变**。比如自动驾驶，你的车变道（Action），后车可能会减速（Environment reaction）。这是最高阶的“环境偏移”。