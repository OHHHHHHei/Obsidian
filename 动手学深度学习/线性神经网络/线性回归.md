# 线性模型 (Linear Model)

我们要根据输入特征（如房屋面积、房龄）预测一个连续值（如房价）。
假设输入特征为 $x_1, x_2, ..., x_d$，权重为 $w_1, w_2, ..., w_d$，偏置为 $b$。

$$price = w_{area} \cdot area + w_{age} \cdot age + b$$

- **权重 ($w$)**：决定了每个特征对结果的影响力（斜率）。

- **偏置 ($b$)**：也叫截距。当所有特征都为0时，预测值是多少。没有它，模型必然经过原点，表达能力受限。

- **仿射变换**：严格来说，加了 $b$ 之后的变换叫仿射变换。如果是纯线性变换，必须满足 $f(0)=0$。

单样本估计
$$\hat{y} = w_1 x_1 + ... + w_d x_d + b$$

这里 $\hat{y}$ 表示对真实值 $y$ 的估计/预测。

向量形式
$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b$$

将所有权重组成向量 $\mathbf{w} \in \mathbb{R}^d$，特征组成向量 $\mathbf{x} \in \mathbb{R}^d$。它们的点积 $\mathbf{w}^\top \mathbf{x}$ 就是 $\sum w_i x_i$。这是最简洁的表达。

矩阵形式 
$$\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + b$$
- $\mathbf{X} \in \mathbb{R}^{n \times d}$：每一行是一个样本，每一列是一个特征。$n$ 是样本数。
- $\mathbf{w} \in \mathbb{R}^{d \times 1}$：权重列向量。
- $\mathbf{X}\mathbf{w}$ 结果为 $n \times 1$ 的向量。
- $b$ 是一个标量。这里利用了**广播机制 (Broadcasting)**，即 $b$ 会被自动复制成一个 $n \times 1$ 的向量，加到 $\mathbf{X}\mathbf{w}$ 的每一个元素上。

# 损失函数

我们需要量化预测值 $\hat{y}$ 和真实值 $y$ 差了多少。

单样本平方损失
$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

**为什么要乘 1/2？**
这是一个数学技巧。当我们对该函数关于 $\hat{y}$ 求导时，指数 2 会下来，与 $\frac{1}{2}$ 抵消，系数变为 1，使导数形式更简洁：$l' = (\hat{y} - y)$。

均方误差 MSE
$$L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^n l^{(i)}(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$$

我们在整个训练集上取平均值。我们的目标就是最小化这个 $L$。

# 解析解

线性回归是一个凸优化问题，且形式简单，可以直接求出精确解。

**步骤**：
1. 为了简化，我们将偏置 $b$ 合并进 $\mathbf{w}$ 中（在 $\mathbf{X}$ 矩阵最后加一列全 1）。
2. 损失函数变为 $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$。
3. 对 $\mathbf{w}$ 求导并令导数为 0。

最优解公式
$$\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}$$

这是最小二乘法的经典公式。

- **局限性**：计算 $(\mathbf{X}^\top \mathbf{X})^{-1}$ 需要矩阵求逆。如果特征维度 $d$ 很大，或者样本数 $n$ 很大，矩阵乘法和求逆的计算量是立方级别的 ($O(d^3)$)，非常慢。因此在深度学习中，我们很少用这个方法。

# 随机梯度下降

# 用模型进行预测

给定“已学习”的线性回归模型， 现在我们可以通过房屋面积和房龄来估计一个（未包含在训练数据中的）新房屋价格。 给定特征估计目标的过程通常称为预测（prediction）或推断（inference）。

# 矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

**核心问题**：Python 是一种解释型语言，`for` 循环的执行开销很大（类型检查、解释器开销）。

**解决方案**：利用线性代数库（如 NumPy, MXNet, PyTorch, TensorFlow）。这些库底层使用 C/C++ 编写，并针对 CPU/GPU 进行了指令集优化（如 SIMD 指令）。

**实验对比**： 课件中对比了两个 10000 维向量相加。
1. **For 循环**：耗时约 **0.16 ~ 9 秒**（取决于框架）。
2. **矢量化 (`+` 运算符)**：耗时约 **0.0002 ~ 0.0006 秒**。

**结论**：矢量化比循环快几个数量级，是深度学习训练必不可少的手段。我们在写代码时，要极力避免使用 `for` 循环来处理数据，而是尽量使用矩阵运算。

# 正态分布与平方损失

正态分布 (高斯分布)：
若随机变量 $x$ 服从均值 $\mu$、方差 $\sigma^2$ 的正态分布，其概率密度函数 (PDF) 为：
$$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$$

线性回归的噪声假设：我们假设了观测中包含噪声，其中噪声服从正态分布。
$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon$$
其中，噪声 $\epsilon \sim \mathcal{N}(0, \sigma^2)$（服从均值为0，方差为 $\sigma^2$ 的正态分布）。

似然函数（Likelihood Function）是统计学中用于量化模型参数与观测数据之间关系的函数。具体来说，对于一个给定的统计模型，似然函数表示在不同参数值下，观测到当前数据的概率。

这意味着，给定输入 $\mathbf{x}$，观测到 $y$ 的概率（似然）服从以预测值为均值的正态分布：
$$P(y|\mathbf{x}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y - \hat{y})^2\right)$$

这里 $\hat{y} = \mathbf{w}^\top \mathbf{x} + b$。

## 最大似然估计

我们要找到一组参数 $\mathbf{w}, b$，使得观测到整个数据集（所有样本）的概率最大。根据独立同分布假设，总似然是所有样本似然的乘积。

为了计算方便（变乘积为求和），我们取负对数似然 (Negative Log-Likelihood, NLL) 并最小化它。
$$-\log P(\mathbf{y}|\mathbf{X}) = -\log \left( \prod_{i=1}^n p(y^{(i)}|\mathbf{x}^{(i)}) \right) = -\sum_{i=1}^n \log p(y^{(i)}|\mathbf{x}^{(i)})$$

代入正态分布公式：

$$= -\sum_{i=1}^n \left( \log \frac{1}{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2}(y^{(i)} - \hat{y}^{(i)})^2 \right)$$

展开并整理：

$$= \sum_{i=1}^n \left( \frac{1}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2}(y^{(i)} - \hat{y}^{(i)})^2 \right)$$

观察上式，第一项 $\frac{1}{2} \log(2\pi\sigma^2)$ 是常数（与 $\mathbf{w}, b$ 无关）。
第二项中，$\frac{1}{2\sigma^2}$ 也是常数。
因此，最小化负对数似然，等价于最小化 $(y^{(i)} - \hat{y}^{(i)})^2$。

这就是为什么我们在 3.1.1 节直接使用平方损失函数的原因——它等价于高斯噪声假设下的极大似然估计。

# 从线性回归到深度网络

现在我们换个角度，用“神经网络”的术语来描述线性回归。

## 神经网络图

![[Pasted image 20251121134120.png]]

神经网络图 (Neural Network Diagram)：

- **输入层**：$x_1, x_2, ..., x_d$。特征维度为 $d$。
- **输出层**：$o_1$。线性回归的输出只是一个标量，所以输出数为 1。
- **层数**：通常只计算有权重的层（计算层），不包括输入层。所以线性回归是**单层神经网络**。

全连接层 (Fully-connected Layer)：

因为每个输入 $x_i$ 都与输出 $o_1$ 相连（参与计算），这种连接方式叫全连接层，也叫稠密层 (Dense Layer)。

# 与生物学的联系

![[Pasted image 20251121134230.png]]

这是一张由树突（dendrites，输入终端）、 细胞核（nucleus，CPU）组成的生物神经元图片。轴突（axon，输出线）和轴突端子（axon terminal，输出端子） 通过突触（synapse）与其他神经元连接。
- **树突 (Dendrites)**：接收输入 ($x_i$)。
- **突触 (Synapse)**：权重 ($w_i$)，决定输入的强弱。
- **细胞核 (Nucleus)**：对加权输入求和 $\sum x_i w_i + b$。
- **轴突 (Axon)**：输出结果 ($y$)，通常会经过激活函数处理。