# 分类问题

独热编码（One-hot Encoding）
我们使用一个向量来表示标签，向量长度等于类别数（这里是3）。
$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$
- $(1, 0, 0)$ 代表“猫”
- $(0, 1, 0)$ 代表“鸡”
- $(0, 0, 1)$ 代表“狗”
- **含义**：在这个向量中，只有对应正确类别的那个位置是 1，其余全为 0。这就是“独热”的由来。

# 网络架构

我们要计算每个输出 $o_1, o_2, o_3$（这里 $o$ 代表 output，通常在深度学习中称为 Logits）。
输入特征有 4 个 ($d=4$)，输出有 3 个 ($q=3$)。
我们需要为每一个输出建立一个线性模型（仿射函数）：
$$ \begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1 \\ o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2 \\ o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3 \end{aligned}$$
- $x_i$：输入特征。
- $w_{ij}$：权重（Weight）。$w_{ij}$ 表示第 $i$ 个输入特征对第 $j$ 个输出类别的影响权重。因为有 4 个输入、3 个输出，所以一共有 $4 \times 3 = 12$ 个权重。
- $b_j$：偏置（Bias）。每个输出都有一个独立的偏置，共 3 个。

矢量化表示
为了书写和计算方便，我们使用线性代数的形式。
- **权重矩阵 $\mathbf{W}$**：形状为 $4 \times 3$（输入维度 $\times$ 输出维度）。
- **输入向量 $\mathbf{x}$**：形状为 $1 \times 4$（行向量）或者 $4 \times 1$（列向量）
- 结果等于 $\mathbf{o} = \mathbf{W}\mathbf{x} + \mathbf{b}$ 

![[Pasted image 20251122161252.png]]
这就是一个**单层神经网络**。
**全连接 (Fully Connected)**：每一个输入节点都连接到了每一个输出节点。
# 全连接的参数开销

 对于 $d$ 个输入和 $q$ 个输出，参数量是 $d \times q$。 在深度学习中，这个数字可能很大。虽然这里提到了一些通过低秩分解（Low-rank decomposition）来减少参数至 $O(dq/n)$ 的方法，但这属于进阶优化，目前大家只需要记住：**全连接层的参数量 = 输入维度 $\times$ 输出维度**。

# Softmax运算

这是本节最关键的概念。
前面计算出的 $o_1, o_2, o_3$ 是什么？它们只是线性变换的结果，范围可能是 $(-\infty, +\infty)$。
比如算出 $o = [0.1, 10.5, -3.2]$。
这不能直接代表概率，因为：
1. 概率不能是负数。
2. 所有类别的概率之和必须等于 1。
我们需要一个“加工厂”，把 $o$ 变成符合概率定义的 $\hat{y}$。这个加工厂就是 **Softmax 函数**。

$$\hat{\mathbf{y}} = \text{softmax}(\mathbf{o})$$

其中第 $j$ 个类别的预测概率 $\hat{y}_j$ 计算如下：

$$\hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$

**为什么要这样做**
1. **分子 $\exp(o_j)$**：使用指数函数 $e^x$。
    - **作用**：将线性输出映射为非负数（因为 $e^x > 0$ 恒成立）。同时，指数函数会拉大数值之间的差距，大的更大，小的更小（这一点对确信度有帮助）。
2. **分母 $\sum_k \exp(o_k)$**：求和。
    - **作用**：归一化（Normalization）。将所有非负数值加起来作为分母，确保所有 $\hat{y}_j$ 除以分母后，加起来的总和严格等于 1。
结果性质：
$$0 \leq \hat{y}_j \leq 1 \quad \text{且} \quad \sum_{j=1}^q \hat{y}_j = 1$$
此时，$\hat{y}$ 就是一个合法的概率分布。

**预测决策**
$$\underset{j}{\operatorname{argmax}} \hat{y}_j = \underset{j}{\operatorname{argmax}} o_j$$
- **解释**：Softmax 运算是单调递增的（$x$ 越大，$e^x$ 越大）。
- **结论**：如果你只关心“是哪个类别”（Hard Classification），而不关心具体的概率值，其实不需要算 Softmax，直接看谁的 $o_j$ 最大，谁就是预测结果。Softmax 主要用于训练阶段（计算损失）和需要输出置信度的场景。
线性模型的本质：
尽管 Softmax 本身是非线性的，但整个模型的决策边界依然是由输入特征的仿射变换（$\mathbf{W}\mathbf{x} + \mathbf{b}$）决定的，所以 Softmax 回归在本质上仍被视为线性模型。


# 小批量样本的矢量化

我们在写代码时，为了利用 GPU 的并行计算能力，通常一次处理 $n$ 个样本（例如 $n=256$）。这就需要把之前的向量运算升级为矩阵运算。
假设我们有一个小批量数据：
- **样本数 (Batch Size)**：$n$
- **特征数 (Input Size)**：$d$
- **类别数 (Output Size)**：$q$
1. 输入矩阵 $\mathbf{X}$：$$\mathbf{X} \in \mathbb{R}^{n \times d}$$每一行是一个样本，每一列是一个特征。
2. 权重矩阵 $\mathbf{W}$：$$\mathbf{W} \in \mathbb{R}^{d \times q}$$
3. 偏置向量 $\mathbf{b}$：$$\mathbf{b} \in \mathbb{R}^{1 \times q}$$注意，偏置是针对“输出节点”的，每个输出节点有一个偏置，所以是一行 $q$ 个。
4. 全连接层运算公式：$$\mathbf{O} = \mathbf{X}\mathbf{W} + \mathbf{b}$$
- 第一步 $\mathbf{X}\mathbf{W}$：
	 $(n \times d)$ 乘以 $(d \times q)$，结果是 $(n \times q)$ 的矩阵。
- 第二步 $+ \mathbf{b}$（关键点）：
	矩阵 $(n \times q)$ 加上向量 $(1 \times q)$，在数学上通常是不允许直接相加的。但在深度学习框架中，会触发 广播机制 (Broadcasting)。
	- **广播含义**：计算机会自动把向量 $\mathbf{b}$ 复制 $n$ 份，变成一个 $(n \times q)$ 的矩阵，然后与 $\mathbf{X}\mathbf{W}$ 对应元素相加。
	- **物理意义**：同一个输出节点的偏置（比如“狗”这一类的偏置），对所有样本都是一样的。
1. Softmax 运算：$$\hat{\mathbf{Y}} = \text{softmax}(\mathbf{O})$$
这里的 Softmax 是按行 (Row-wise) 执行的。也就是说，我们对每一个样本（每一行）单独计算概率分布。
- $\hat{\mathbf{Y}}$ 的形状也是 $n \times q$，每一行的和为 1。

# 损失函数

现在模型输出了预测概率 $\hat{\mathbf{y}}$，我们也有真实标签 $\mathbf{y}$。我们需要一个函数来量化它们之间的差距。

## 对数似然

【核心思想：最大似然估计】
我们要优化的目标很直观：让模型预测出“正确标签”的概率尽可能大。
假设样本 $i$ 的真实类别是“猫”，模型的预测输出是 $[\text{狗}=0.1, \text{猫}=0.8, \text{鸡}=0.1]$。
我们希望 $P(\text{猫} | \text{特征})$ 越大越好（这里是 0.8）。

单样本概率表示：
利用独热编码 $\mathbf{y}$（只有正确位置为 1，其余为 0）和预测概率 $\hat{\mathbf{y}}$。
正确类别的预测概率可以写成：
$$P(y | \mathbf{x}) = \prod_{j=1}^q \hat{y}_j^{y_j}$$
- _解释_：因为 $y_j$ 只有在正确类别时为 1，其他为 0。任何数的 0 次方都是 1。所以这个连乘式子最后只留下了**正确那一类的预测概率**。
- 例子：真实是第 2 类 $\mathbf{y}=[0, 1, 0]$，预测 $\hat{\mathbf{y}}=[0.1, 0.8, 0.1]$。$$P = 0.1^0 \times 0.8^1 \times 0.1^0 = 1 \times 0.8 \times 1 = 0.8$$
假设样本独立同分布，整个数据集的联合概率（似然）是所有样本概率的乘积：

$$P(\mathbf{Y} | \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} | \mathbf{x}^{(i)})$$
取负对数：
直接优化乘积很难（数值容易溢出变成 0），所以我们取对数 $\log$，把乘法变成加法。
同时，因为优化算法通常是“最小化”损失，而我们想“最大化”概率，所以加个负号。
$$-\log P(\mathbf{Y} | \mathbf{X}) = -\log \left( \prod_{i=1}^n P(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}) \right) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} | \mathbf{x}^{(i)})$$
这等价于最小化：
$$\sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})$$
## 交叉熵损失 (Cross-Entropy Loss)

把上面的单样本逻辑代入，我们得到了大名鼎鼎的 交叉熵损失函数：
$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j$$

为什么它这么重要？
因为 $\mathbf{y}$ 是独热向量，假设真实类别是 $k$，那么只有 $y_k=1$，其他 $y_j=0$。
公式瞬间简化为：
$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \log \hat{y}_k$$
即：损失值 = - log(预测出的正确类别的概率)
**【直观理解】**
- 如果模型预测正确类别的概率 $\hat{y}_k = 1$（非常确信），$\text{Loss} = -\log(1) = 0$（完美，无损失）。
- 如果模型预测正确类别的概率 $\hat{y}_k = 0.1$（很不确信），$\text{Loss} = -\log(0.1) \approx 2.3$（惩罚较大）。
- 如果模型预测正确类别的概率 $\hat{y}_k \rightarrow 0$（完全错了），$\text{Loss} \rightarrow +\infty$（惩罚无限大）。
这就是交叉熵损失如何迫使模型去提高正确类别的预测概率。

# Softmax及交叉熵损失的导数

我们在上一部分定义了损失函数 $l = -\sum y_j \log \hat{y}_j$。
为了通过梯度下降更新权重 $\mathbf{W}$，我们需要求损失函数 $l$ 对输入 logits $\mathbf{o}$ 的导数 $\partial l / \partial o_j$。
虽然 Softmax ($\frac{e^{o_j}}{\sum e^{o_k}}$) 和 log ($\log \hat{y}$) 分开求导都很麻烦，但它们组合在一起时，通过链式法则推导，复杂项会互相抵消。
- 最终结论：$$\frac{\partial l}{\partial o_j} = \text{softmax}(\mathbf{o})_j - y_j = \hat{y}_j - y_j$$
- **含义**：损失函数对第 $j$ 个输出节点 $o_j$ 的梯度，竟然直接等于 **预测概率 $\hat{y}_j$ 减去真实标签 $y_j$**。
	
- **直观理解**：
	- 假设真实是第 1 类 ($y_1=1$)，预测概率 $\hat{y}_1=0.8$。梯度 = $0.8 - 1 = -0.2$。负梯度表示我们需要增大 $o_1$。
	- 假设真实不是第 2 类 ($y_2=0$)，预测概率 $\hat{y}_2=0.3$。梯度 = $0.3 - 0 = 0.3$。正梯度表示我们需要减小 $o_2$。
	- 差异越大，梯度越大，更新步长越大。
【为什么这样做？】
这种导数形式与线性回归中的（预测值 - 真实值）完全一致。这说明 Softmax + 交叉熵不仅符合概率逻辑，在优化计算上也极其高效。


# 信息论基础

## **熵 (Entropy)**
- **概念**：衡量一个分布的信息量或不确定性。如果不确定性越大（比如每个类别发生的概率都一样），熵就越大。
$$H[P] = \sum_j - P(j) \log P(j)$$

- **纳特 (nat)**：当对数底为 $e$ 时，信息单位叫“纳特”；底为 2 时叫“比特 (bit)”。
## **信息量**

- 事件发生概率越低，发生时我们就越“惊讶”，包含的信息量就越大。例如“太阳从西边出来”（概率极低）比“太阳从东边出来”（概率极高）包含更多信息。
- 信息量定义为 $-\log P(j)$。
## **重新审视交叉熵 (Revisiting Cross-Entropy)**

- **定义**：交叉熵 $H(P, Q)$ 衡量了用概率分布 $Q$（我们的预测）来编码服从概率分布 $P$（真实标签）的数据所需的平均编码长度。$$H(P, Q) = H(P) + D_{KL}(P \| Q)$$
    其中 $H(P)$ 是真实数据的熵（对于确定的独热标签，这是 0），$D_{KL}$ 是 KL 散度 (Kullback-Leibler Divergence)，衡量两个分布的距离。
- **结论**：最小化交叉熵 $H(P, Q)$，本质上就是最小化预测分布 $Q$ 与真实分布 $P$ 之间的距离 ($D_{KL}$)。

# 模型预测和评估 (Prediction and Evaluation)

训练结束后，我们不再关注损失值，而是关注模型到底“猜对”了没有。
- 预测决策：选择概率最大的类别作为预测结果。$$\text{预测类别} = \underset{j}{\operatorname{argmax}} \hat{y}_j$$
- 评估指标：精度 (Accuracy)$$\text{精度} = \frac{\text{预测正确的样本数}}{\text{总样本数}}$$
- 注意：在分类任务中，即使 Loss 还在下降（概率从 0.6 变到 0.9），精度可能保持不变（因为 argmax 还是同一个类别）。精度是离散的，Loss 是连续的。