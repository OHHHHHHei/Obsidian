# 加载和保存张量

## 保存单个张量
```Python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')
```

- **代码解读**：
    - `x = torch.arange(4)`：我们在内存中创建了一个向量 `[0, 1, 2, 3]`。
    - `torch.save(x, 'x-file')`：
        - 第一个参数 `x`：是我们要保存的对象。
        - 第二个参数 `'x-file'`：是保存的文件路径和名称。
    - **原理**：PyTorch 会调用 Python 的 `pickle` 工具，将这个对象序列化成二进制流，写入磁盘
## 加载单个张量
```Python
x2 = torch.load('x-file')
x2
```
- **输出**：`tensor([0, 1, 2, 3])`
- **详细讲解**：`torch.load` 函数读取磁盘上的二进制文件，并将其反序列化，在内存中重建一个张量对象。你可以看到，`x2` 的内容和 `x` 完全一致。
## 保存张量列表

实际应用中，我们要保存的往往不是一个数，而是一组数。比如你可能想同时保存“训练集的Loss”和“验证集的Loss”。我们可以直接保存一个**列表（List）**。
```Python
y = torch.zeros(4)
torch.save([x, y], 'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
```

- **代码解读**：
    - 这里 `y` 是一个全0向量。
    - `[x, y]` 是 Python 的列表。
    - `torch.save` 非常强大，它不仅能存 Tensor，还能存包含 Tensor 的 Python 列表。
    - 读取时，`torch.load` 返回的也是一个列表，我们可以直接用 `x2, y2` 进行解包赋值。
    - **结果**：`x2` 恢复为 `[0, 1, 2, 3]`，`y2` 恢复为 `[0., 0., 0., 0.]`。
## 保存张量字典

这是最实用的部分。当我们要保存模型参数时，光有一堆数字是不够的，我们需要知道 **“哪个参数叫什么名字”**。比如，“这是第一层的权重”还是“第二层的偏置”。

这时候，**字典（Dictionary)** 就是最好的载体。
```Python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```

- **详细讲解**
    - 我们创建了一个字典，字符串 `'x'` 映射到张量 `x`，字符串 `'y'` 映射到张量 `y`。
    - 这一步非常关键，它是下一节“保存模型参数”的底层逻辑——所谓的模型参数（`state_dict`），本质上就是一个巨大的、也就是这里演示的**从字符串（参数名）映射到张量（参数值）的字典**。
# 加载和保存模型参数

## **第一步：定义并初始化模型**

首先，我们需要一个网络。课件中定义了一个简单的 MLP
- 输入层维度：20
- 隐藏层维度：256（激活函数 ReLU）
- 输出层维度：10
```Python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20)) # 生成一个随机输入
Y = net(X) # 得到预测结果 Y
```
## **第二步：保存模型参数（重点）**

注意听，这里有一个自动化专业必须掌握的术语：**`state_dict`**。
在 PyTorch 中，模型的所有可学习参数（权重和偏置）都保存在一个有序字典中，叫做 `state_dict`。
```Python
torch.save(net.state_dict(), 'mlp.params')
```

- **深度解析**
- `net.state_dict()`：这一步把网络里所有的 `hidden.weight`, `hidden.bias`, `output.weight` 等等提取出来，打包成一个字典。
- `torch.save(..., 'mlp.params')`：把这个字典序列化写入硬盘。
- **文件名**：课件用了 `.params`，但在 PyTorch 社区，我们通常习惯用 `.pth` 或 `.pt`。
## **第三步：恢复模型**

很多同学在这里会犯错。他们以为 `load` 回来的就是一个模型对象。**错！**
因为我们只保存了参数（数据），没有保存代码（类的定义）。所以恢复的过程分两步：
1. **搭建骨架**：你必须先在代码里实例化一个一模一样的网络结构。
2. **注入灵魂**：把加载回来的参数填进去。
- **操作代码**：
```Python
clone = MLP() # 1. 重新实例化一个新模型，此时参数是随机初始化的
clone.load_state_dict(torch.load('mlp.params')) # 2. 加载硬盘上的参数，覆盖随机参数
clone.eval() # 3. 切换到评估模式
```
- **为什么要 `clone.eval()`？**：
- 虽然这个简单的 MLP 没有 Dropout 或 Batch Normalization 层，但在工程规范中，加载模型用于预测（Inference）时，必须调用 `eval()`。这告诉模型：“别再学习了，也别随机扔掉神经元了，稳定输出！”
## **第四步：验证一致性**
如何证明 `clone` 和原来的 `net` 是一样的？ 我们把同样的输入 `X` 喂给 `clone`，看看输出的 `Y_clone` 是否等于原来的 `Y`。
```Python
Y_clone = clone(X)
Y_clone == Y
```
- **结果**：全是 `True`。证明参数完美复原。
# 问题
- **问题：即使不需要部署到不同设备，存储模型参数还有什么好处？**
    - **讲解**：
        - **Checkpoint（断点续训）**：防止训练崩了白跑。
        - **Early Stopping（早停）**：训练过程中，我们通常保存验证集误差最小的那一版参数，而不是最后一轮的参数。
        - **A/B Test**：保存不同超参数下的模型进行对比。
- **问题：假设我们只想复用网络的一部分（例如前两层），怎么做？**
    - **讲解**：这是**迁移学习（Transfer Learning）**的基础。
    - **操作逻辑**：
        1. 读取完整的 `state_dict`。
        2. 创建一个新的 `state_dict`，只保留你需要的键值对（Key-Value），过滤掉不需要的层（比如最后一层全连接层）。
        3. 使用 `load_state_dict(new_dict, strict=False)` 加载到新模型中。`strict=False` 允许参数不完全匹配。
- **问题：如何同时保存网络架构和参数？**
    - **讲解**：
        - PyTorch 支持 `torch.save(net, 'model.pth')`（保存整个对象）。
        - **限制**：这依赖于 Python 的 pickle 机制。恢复模型时，你的代码目录下必须有定义 `class MLP` 的原始文件。如果你把代码发给别人，或者改了类名，加载就会报错。所以工业界**极度不推荐**这种做法，还是推荐只存 `state_dict`。