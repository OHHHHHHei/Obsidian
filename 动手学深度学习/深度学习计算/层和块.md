- **从简单到复杂**：课程回顾了之前学过的简单神经网络（输入->输出）。
- **引入问题**：当网络变得非常深（如 ResNet 有上百层）时，单独管理每一层非常痛苦且容易出错。
- **提出解决方案**：引入 **“块 (Block)”** 的概念。块是一个抽象层级，它比单个神经元大，但比整个模型小。
# **块的定义与抽象**

- 最底层是**层 (Layer)**：比如一个全连接层。
- 中间层是**块 (Block)**：这就好比把几个层打包成一个组件。
- 最顶层是**模型 (Model)**：由多个块组成。
- **工程实现**：
    - 在编程语言（如 Python）中，**块由 类 (Class) 表示**。
    - 在 PyTorch 中，所有神经网络的基类是 `nn.Module`
    - **前向传播 (Forward)**：这是你必须自己写的函数，定义数据怎么从输入变成输出。
    - **反向传播 (Backward)**：得益于自动微分（Autograd），我们通常不需要自己写，框架会帮我们算梯度。
![[Pasted image 20251130232214.png]]
## 回顾 MLP 的 Sequential 实现

文档中给出了一个利用 Sequential 快速搭建 MLP 的例子。我们以 PyTorch 代码为例进行详细剖析：
```Python
import torch
from torch import nn
from torch.nn import functional as F

# 实例化一个 Sequential 块
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

# 生成随机输入 X
X = torch.rand(2, 20)
# 前向传播
net(X)
```

**代码详解与公式对应**：
1. **`nn.Sequential(...)`**：
    - 这是一个特殊的**容器块**。它的逻辑非常简单：按顺序把括号里的东西串起来。
    - **第一层 `nn.Linear(20, 256)`**：
        - 输入维度 $d_{in} = 20$，输出维度 $d_{hidden} = 256$。
        - 数学运算：$\mathbf{H} = \mathbf{X}\mathbf{W}_1 + \mathbf{b}_1$。
        - 输入 $\mathbf{X}$ 的形状是 $(2, 20)$（Batch size=2），经过这层后形状变为 $(2, 256)$。
    - **第二层 `nn.ReLU()`**：
        - 激活函数。
        - 数学运算：$\mathbf{A} = \max(0, \mathbf{H})$。
        - 形状保持不变，仍为 $(2, 256)$。
    - **第三层 `nn.Linear(256, 10)`**：
        - 输入维度 $256$，输出维度 $10$。
        - 数学运算：$\mathbf{O} = \mathbf{A}\mathbf{W}_2 + \mathbf{b}_2$。
        - 最终输出形状为 $(2, 10)$。
2. **`net(X)`**：
    - 这一行代码其实是 Python 的语法糖。它实际上调用了 `net.__call__(X)`。
    - 在 `nn.Module` 的设计中，`__call__` 会自动调用 `forward(X)` 函数。
    - **为什么这样做？** 这样可以让对象像函数一样被调用，代码更简洁。

# 自定义块

## 块的 5 个必备功能

任何一个合格的“块”（无论是层、模型还是组件），都必须能做以下 5 件事：
1. **接收输入**：就像函数接收参数一样。
2. **生成输出**：通过前向传播函数计算结果。注意，输出形状可能和输入不一样（比如输入 20 维，输出 256 维）。
3. **计算梯度**：这通常由自动微分机制自动完成，我们不需要手写反向传播。
4. **存储参数**：比如全连接层的权重 $W$ 和偏置 $b$，块需要把它们存好，并能让我们访问到。
5. **初始化参数**：给参数赋初始值（比如随机初始化）。

**代码详解：手写 MLP** 我们要用 Python 类来实现上一节那个 MLP）：
```python
class MLP(nn.Module):
    # 1. 声明带有模型参数的层
    def __init__(self):
        # 2. 调用父类的初始化函数
        super().__init__()
        # 3. 定义隐藏层：全连接，输入20，输出256
        self.hidden = nn.Linear(20, 256)
        # 4. 定义输出层：全连接，输入256，输出10
        self.out = nn.Linear(256, 10)

    # 5. 定义前向传播：如何利用上面的层把输入变成输出
    def forward(self, X):
        # 注意：ReLU 激活函数在这里使用
        return self.out(F.relu(self.hidden(X)))
```
- **`class MLP(nn.Module):`**
    - **继承**：我们自定义的类 `MLP` 继承自 `nn.Module`。这是 PyTorch 中所有神经网络模块的基类。它提供了参数管理、GPU 移动等底层功能。
- **`def __init__(self):`**
    - 这是构造函数。当我们在外面写 `net = MLP()` 时，就会运行这里的代码。
    - **`super().__init__()`**：**极度重要！** 这行代码调用了父类 `nn.Module` 的构造函数。它会初始化一系列内部变量（比如用于存储参数的字典）。如果你忘了写这行，PyTorch 就不知道这个类是一个神经网络模块，训练时会报错。
    - **`self.hidden = ...`** 和 **`self.out = ...`**：
        - 我们在这里实例化了两个具体的层，并把它们赋值给 `self` 的属性。
- **`def forward(self, X):`**
    - 这是前向传播的核心逻辑。参数 `X` 就是输入数据。
    - **`self.hidden(X)`**：数据 `X` 经过隐藏层，计算线性变换。
    - **`F.relu(...)`**：对线性变换的结果应用 ReLU 激活函数。注意，这里用的是函数式的 ReLU (`F.relu`)，而不是类形式的 (`nn.ReLU`)，因为 ReLU 没有参数，直接调用函数更方便。
    - **`self.out(...)`**：将激活后的结果传入输出层。
    - **`return ...`**：返回最终结果。
# 顺序块

- **设计思路**：
    1. 在初始化时，把所有传入的层存到一个特殊的列表中。
    2. 在前向传播时，用一个 `for` 循环遍历这个列表，把数据一层层传下去。
- **关键技术**：如何存储这些层？普通的 Python `list` 是不行的，必须用 PyTorch 特定的存储方式（`_modules`），否则训练时找不到参数。
## **手写 MySequential**
```Python
class MySequential(nn.Module):
    # *args 表示可以接收任意数量的参数，比如 net = MySequential(layer1, layer2, layer3)
    def __init__(self, *args):
        super().__init__()
        # 遍历传入的每一个 block (层)
        for idx, module in enumerate(args):
            # 重点！我们把 module 存进了 self._modules 这个特殊的字典里
            # key 是索引的字符串形式（"0", "1", ...），value 是具体的层
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict 保证了我们按添加的顺序遍历它们
        for block in self._modules.values():
            # 把上一层的输出作为下一层的输入
            X = block(X)
        return X
```

**逐行解析：
1. **`for idx, module in enumerate(args):`**
    - 这里遍历用户传入的所有层。
2. **`self._modules[str(idx)] = module`** （**核心考点**）
    - 为什么不直接写 `self.layers = list(args)`？”
    - **原因**：`nn.Module` 父类有一个机制，它只会自动扫描 `self._modules` 字典（或者以 `self.xxx = layer` 形式定义的属性）来寻找需要训练的参数。
    - 如果你把层只是放进一个普通的 Python `list` 中，PyTorch 会认为那只是普通的变量，**不会去计算它们的梯度**，训练时参数就不会更新。
    - 所以，我们必须显式地把层加入到 `self._modules` 中。
3. **`def forward(self, X):`**
    - 这里的逻辑非常通用。不管网络有多少层，它只是简单地做一个接力跑：拿着输入 `X`，跑过所有的 `block`，最后返回结果。
# 在前向传播函数中执行代码

- **提出挑战**：`Sequential` 只能线性堆叠。如果我们需要一个**常数参数**（不参与训练），或者需要根据数据的值动态决定计算路径（**控制流**），该怎么办？
- **解决方案**：在自定义块的 `forward` 函数中，利用 Python 语言的灵活性，加入数学运算、循环和条件判断。
## **包含控制流的 MLP**

```Python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 1. 定义一个【常数参数】
        # torch.rand 生成随机数，但 requires_grad=False 表示它不需要计算梯度
        # 这意味着在训练过程中，这个矩阵的值永远不会变，它只是一个固定的数学参数
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        
        # 2. 定义一个普通的线性层（这也是一个参数，是可以学习的）
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        # 步骤 A: 先过一遍普通的线性层
        X = self.linear(X)
        
        # 步骤 B: 手动实现一个全连接运算
        # torch.mm 是矩阵乘法 (Matrix Multiplication)
        # 这里计算: X * W_const + 1，然后过 ReLU
        # 这一步虽然有计算，但因为 rand_weight 不可导，所以不会更新 rand_weight
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        
        # 步骤 C: 再过一遍线性层（复用同一个层，相当于参数共享）
        X = self.linear(X)
        
        # 步骤 D: 【控制流】
        # 这里用 Python 的 while 循环检查输出 X 的 L1 范数（绝对值之和）
        # 如果总和大于 1，就不断把 X 除以 2，直到它变小为止
        while X.abs().sum() > 1:
            X /= 2
            
        # 步骤 E: 返回标量（所有元素的和）
        return X.sum()
```

**解析：为什么这样做？**

- **`requires_grad=False`**：在工程中，这常用于**冻结部分网络**（例如迁移学习时冻结骨干网络），或者引入物理模型中的固定系数矩阵。
    
- **`while` 循环**：这展示了动态计算图的优势。对于每个不同的输入 `X`，循环执行的次数可能不同。PyTorch 会自动追踪这个动态过程并正确计算梯度（对于 `X /= 2` 操作）。
## **混合嵌套：俄罗斯套娃** 除了写复杂的逻辑，我们还可以把各种块随意组合。

```Python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 在自定义块里，套一个 Sequential
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

# 【超级组合】
# 1. NestMLP (自定义块)
# 2. nn.Linear (标准层)
# 3. FixedHiddenMLP (带控制流的自定义块)
chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
# 前向传播
chimera(X)
```

“看这个 `chimera`（嵌合体）网络。它证明了**层是块，Sequential 是块，自定义类也是块**。它们接口统一，可以任意互相嵌套。这就好比你写自动化程序，子程序可以调用子程序，也可以调用库函数，非常灵活。”