# 参数访问

- **引言**：在定义好网络架构（如上一节学的 `Sequential`）后，下一步就是训练。训练的本质就是寻找最优的参数（权重和偏置）以最小化损失函数。
- **需求**：在训练、预测或保存模型时，我们需要能够“拿到”这些参数。
- **单层访问**：网络可以看作一个列表（List），可以通过索引（如 `net[1]`）访问特定的层。
- **参数对象**：每一层不仅包含数值，还包含梯度等信息，所以参数是一个对象（Object/Class），而不只是一个纯数字矩阵。我们需要学会从对象中提取数值（`data`）和梯度（`grad`）。
- **嵌套访问**：当网络结构变复杂（层里套层）时，需要通过递归或特定命名方式来定位参数。

课件首先定义了一个单隐藏层的 MLP：
```Python
import torch
from torch import nn

# 定义网络：输入4维 -> 隐藏层8维 (ReLU) -> 输出1维
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
# 生成随机输入 X，形状 (2, 4)，即 batch_size=2, 特征数=4
X = torch.rand(size=(2, 4))
# 前向传播，生成输出
net(X)
```
- **解释**：这里定义了三层。
    - 索引 0: `Linear(4, 8)`
    - 索引 1: `ReLU()`
    - 索引 2: `Linear(8, 1)`
    - **数学逻辑**：第一层计算 $H = XW_1^T + b_1$，第二层 $O = \text{ReLU}(H)W_2^T + b_2$。
## 访问特定层

```Python
# 访问索引为2的层（即输出层 Linear(8, 1)）的参数状态字典
print(net[2].state_dict())
```
- **输出解释**：
    - 输出是一个 `OrderedDict`（有序字典）。
    - 包含 `'weight'` 和 `'bias'` 两个键。
    - **维度推导（重点）**：
        - `weight` 的形状是 `[1, 8]`。**为什么？**
        - 在 PyTorch 的 `nn.Linear(in_features, out_features)` 中，权重矩阵在内部存储时的形状是 `(out_features, in_features)`。这是为了方便计算 $y = xA^T + b$。这里输出维度是 1，输入是 8，所以是 $1 \times 8$。
        - `bias` 的形状是 `[1]`，对应输出节点的个数。
## 提取数值与梯度

如果我们想具体操作某个参数（比如只看偏置），不能只打印字典，要直接访问属性。
```Python
# 访问第2层（输出层）的 bias 属性
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
```
- **详细解读**：
    1. `type(net[2].bias)`：返回 `<class 'torch.nn.parameter.Parameter'>`。
        - **为什么要这样做？** PyTorch 不直接用 Tensor 存储参数，而是用 `Parameter` 类包装。这样做是为了让自动微分机制（AutoGrad）能识别出“这是一个需要优化的参数，而不是普通的输入数据”。
    2. `net[2].bias`：打印对象，显示 `tensor([0.0887], requires_grad=True)`。
        - `requires_grad=True` 意味着如果你对 `loss` 求导，这个参数会计算梯度。
    3. `net[2].bias.data`：访问底层的 Tensor 数值。在旧版本 PyTorch 中常用于更新参数而不影响梯度图（现在推荐用 `with torch.no_grad():`）
- **关于梯度（Gradient）**：
```    Python
net[2].weight.grad == None
```
- **解释**：输出为 `True`。
- **原因**：我们只进行了前向传播 `net(X)`，**还没有进行反向传播**（`loss.backward()`）。在 PyTorch 中，梯度是懒加载的，只有反向传播后，`.grad` 属性才会有数值。
## 一次性访问与嵌套块

当网络很深时，我们需要批量查看参数。
```Python
# 访问 net[0] (即第一个 Linear 层) 的命名参数
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
# 访问整个 net 的命名参数
print(*[(name, param.shape) for name, param in net.named_parameters()])
```
- **逻辑**：
    - `named_parameters()` 是一个迭代器，返回 `(名字, 参数张量)` 的元组。
    - **对于 `net[0]`**：名字是 `'weight'` 和 `'bias'`。
    - **对于 `net`**：PyTorch 会自动加上层级的索引作为前缀。
        - 结果显示：`('0.weight', ...), ('0.bias', ...), ('2.weight', ...)`。
        - **注意**：索引 1 是 ReLU，因为它没有参数（Parameter），所以不会出现在这个列表中。
- 嵌套块（Nested Blocks）逻辑：
    课件定义了一个生成块的函数 block1 和 block2，并将它们嵌套到 rgnet 中。
    - `rgnet` 包含了 `block2`，`block2` 包含了 4 个 `block1`，`block1` 包含了 `Linear` 层。
    - **访问方式**：像访问多维数组一样。
```   Python
rgnet[0][1][0].bias.data
```
- **推导**：
	- `rgnet[0]` -> 也就是 `block2`。
	- `block2[1]` -> `block2` 里的第 2 个子模块（即第 2 个 `block1`）。
	- `block1[0]` -> `block1` 里的第 1 个层（即 `Linear(4, 8)`）。
	- `.bias.data` -> 取出该层的偏置数值。
# 参数初始化

- **承接上文**：上一节我们学会了怎么找到参数（寻址），这一节我们学习怎么给它们赋值（写值）。
- **默认机制**：深度学习框架（如 PyTorch）通常有默认的随机初始化策略（通常是均匀分布），这能保证模型一开始能“动”起来。
- **内置初始化**：为了更好的效果（例如避免梯度消失或爆炸），我们需要使用统计学上更优的分布（如高斯分布、Xavier 初始化）来覆盖默认值。
- **自定义初始化**：如果内置的方法不满足需求（比如我非要让权重等于一个特定的概率分布），我们需要编写自定义函数。
- **应用机制**：框架提供了一种“遍历应用”的机制（`net.apply`），

## 内置初始化

将所有权重初始化为标准差为 0.01 的高斯分布（正态分布），并将偏置设为 0。
```Python
def init_normal(m):
    # 1. 筛选：只对全连接层（Linear）进行操作
    if type(m) == nn.Linear:
        # 2. 权重初始化：使用正态分布，均值 mean=0, 标准差 std=0.01
        # 注意：normal_ 后面有个下划线，代表 In-place 操作（原地修改）
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # 3. 偏置初始化：设为 0
        nn.init.zeros_(m.bias)

# 4. 广播指令：将 init_normal 函数应用到 net 的每一层
net.apply(init_normal)

# 5. 验证结果
print(net[0].weight.data[0], net[0].bias.data[0])
```
- **解释**：
    - **`m` 是什么？** 当 `net.apply(init_normal)` 运行时，它会把 `net` 里的每一层（Layer）依次传给 `init_normal` 函数，这个 `m` 就代表当前的某一层（Module）。
    - **`type(m) == nn.Linear`**：这是一个安全锁。因为 `net` 里可能还有 `ReLU`、`Dropout` 等没有参数的层，或者 `BatchNorm` 等需要不同初始化策略的层。如果不加判断直接访问 `m.weight`，代码可能会报错。
## 使用常量初始化

有时为了调试，我们想把所有参数设为同一个数（比如 1）。
```Python
def init_constant(m):
    if type(m) == nn.Linear:
        # 将权重全部设为 1
        nn.init.constant_(m.weight, 1)
        # 将偏置全部设为 0
        nn.init.zeros_(m.bias)

net.apply(init_constant)
```
- **注意**：在实际训练中，**绝对不能**将隐藏层的权重全部初始化为常数（比如全 1 或全 0）。这会导致“对称性破坏”（Symmetry Breaking）失败，所有神经元学到的东西一模一样，网络退化成一个单神经元模型。
## 混合初始化

我们可以对不同的层使用不同的策略。
```Python
def init_xavier(m):
    if type(m) == nn.Linear:
        # Xavier 初始化（也叫 Glorot 初始化），一种根据输入输出节点数自动调整方差的方法
        nn.init.xavier_uniform_(m.weight)

def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

# 对第一层（索引0）使用 Xavier 初始化
net[0].apply(init_xavier)
# 对第三层（索引2）使用常数 42 初始化
net[2].apply(init_42)
```
- **逻辑**：`apply` 方法不仅可以对整个 `net` 调用，也可以对单独的层 `net[0]` 调用。这提供了极大的灵活性。
## 自定义初始化

如果内置的 nn.init 都不满足需求，我们可以自己写数学公式。
$$w \sim \begin{cases} U(5, 10) & \text{可能性 } \frac{1}{4} \\ 0 & \text{可能性 } \frac{1}{2} \\ U(-10, -5) & \text{可能性 } \frac{1}{4} \end{cases}$$
这意味着我们希望参数保留较大的绝对值（大于5），或者保留为0（稀疏化）。
```Python
def my_init(m):
    if type(m) == nn.Linear:
        # 1. 打印名字和形状（用于调试）
        print("Init", *[(name, param.shape) for name, param in m.named_parameters()][0])
        
        # 2. 先用均匀分布 U(-10, 10) 填充
        nn.init.uniform_(m.weight, -10, 10)
        
        # 3. 逻辑过滤：保留绝对值 >= 5 的，其他的设为 0
        # m.weight.data.abs() >= 5 会生成一个 True/False 的掩码矩阵
        # m.weight.data *= ...  利用 Python 的布尔乘法（True=1, False=0）实现过滤
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
```
- **解释**：这段代码展示了我们可以直接对 `m.weight.data` 进行张量运算，实现任意复杂的逻辑。
## **直接参数修改**

最简单粗暴的方法：像操作数组一样直接修改。
```Python
# 将第一层的权重整体 +1
net[0].weight.data[:] += 1
# 将第一层的权重的第1行第1列设为 42
net[0].weight.data[0, 0] = 42
```
- **应用场景**：这通常用于微调（Fine-tuning）时手动干预某个参数，或者在迁移学习中加载部分预训练权重。

# 参数绑定

- **需求**：有时候我们希望网络的不同层使用完全相同的参数。这不仅是为了节省内存，更是为了给模型施加某种约束（例如：无论输入信号在序列的哪个位置，处理它的规则应该是相同的，这是卷积神经网络CNN和循环神经网络RNN的核心思想）。
- **实现方式**：不能简单地定义两个形状一样的层，而是要实例化**一个**层对象，然后把它多次添加到网络的不同位置。
- **底层机制**：这两个层在 Python 层面指向内存中的**同一个对象**（引用传递）。
- **梯度行为**：这是重点。当参数共享时，反向传播会发生什么？因为损失函数对该参数的导数来自多条路径，根据微积分的链式法则，这些梯度会**累加**。
```Python
# 1. 先实例化一个具体的层，命名为 shared
shared = nn.Linear(8, 8)

# 2. 将这个 shared 层多次放入 Sequential 中
net = nn.Sequential(
    nn.Linear(4, 8),  # 索引 0: 普通层
    nn.ReLU(),        # 索引 1: 激活
    shared,           # 索引 2: 共享层 (第一次出现)
    nn.ReLU(),        # 索引 3: 激活
    shared,           # 索引 4: 共享层 (第二次出现)
    nn.ReLU(),        # 索引 5: 激活
    nn.Linear(8, 1)   # 索引 6: 输出层
)
```

- **结构解析**：
    - 这个网络有 4 个线性层（Linear），但实际上只占用了 3 份参数的内存空间。
    - 第 2 层（隐藏层2）和第 4 层（隐藏层3）是**完全一样**的。它们不仅权重数值一样，连内存地址都是一样的。
**【前向传播与验证】**
```Python
X = torch.rand(size=(2, 4))
net(X)

# 验证1：检查数值是否相等
print(net[2].weight.data[0] == net[4].weight.data[0])
# 输出: True

# 验证2：修改其中一个，看另一个变不变
net[2].weight.data[0, 0] = 100
print(net[4].weight.data[0, 0] == 100)
# 输出: True
```
- **现象解释**：
    - 当你执行 `net[2].weight.data[0, 0] = 100` 时，你修改的是 `shared` 这个对象内部的数据。
    - 因为 `net[4]` 指向的也是 `shared` 这个对象，所以读取 `net[4]` 时，自然也是 100。
    - 这就像你给了两个人（`net[2]` 和 `net[4]`）同一把钥匙，开的是同一扇门。