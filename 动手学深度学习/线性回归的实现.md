# 生成数据集
```Python
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

# 读取数据集

训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。

我们定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。 每个小批量包含一组特征和标签。
```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。

# 初始化模型参数

在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。
```python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```
在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。
# 定义模型

我们现在要定义模型，将模型的输入和参数同模型的输出关联起来。
```python
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```
因为需要计算损失函数的梯度，所以我们应该定义损失函数
```python
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

# 定义优化算法

在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。 该函数接受模型参数集合、学习速率和批量大小作为输入。每 一步更新的大小由学习速率`lr`决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（`batch_size`） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。
**代码解析**：
```Python
def sgd(params, lr, batch_size):
    """小批量随机梯度下降"""
    # torch.no_grad() 表示在这个代码块内，不要记录计算图
    # 因为更新参数本身不需要求导，我们只是在修改数值
    with torch.no_grad():
        for param in params:
            # param.grad 存储了之前 backward() 计算出的梯度
            # 注意：我们要除以 batch_size，因为损失函数通常是求和形式（或者梯度是求和形式）
            # 如果损失函数已经是平均值 (mean)，这里就不需要除以 batch_size
            # 但书中的 squared_loss 是求和形式（没有除以 n），所以这里要规范化
            param -= lr * param.grad / batch_size
            
            # 清零梯度！非常重要！
            # 否则下一次 backward() 时，梯度会累加到旧梯度上
            param.grad.zero_()
```

# 训练

在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法`sgd`来更新模型参数。

初始化参数
重复以下训练直到完成
计算梯度：$\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$。
更新参数：$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$
主代码如下
```python
# === 超参数设置 ===
lr = 0.03          # 学习率 (Learning Rate)
num_epochs = 3     # 迭代周期数 (扫几遍数据)
net = linreg       # 我们的模型: Xw + b
loss = squared_loss # 我们的损失函数: 0.5 * (y_hat - y)^2

# === 训练主循环 ===
for epoch in range(num_epochs):
    # 每次从数据集中拿出一个 batch (X, y)
    for X, y in data_iter(batch_size, features, labels):
        
        # --- 1. 前向传播 (Forward) ---
        # 计算预测值 y_hat，并计算损失 l
        # net(X, w, b) 得到的是 (batch_size, 1) 的预测向量
        # l 也是一个 (batch_size, 1) 的向量，包含了每个样本的损失
        l = loss(net(X, w, b), y)  
        
        # --- 2. 反向传播 (Backward) ---
        # 因为 l 是一个向量，PyTorch 的 backward() 只能对标量进行。
        # 所以我们先求和 l.sum()，得到这批样本的总损失。
        # 求导后，w.grad 中存储的是“这批样本梯度的和”。
        l.sum().backward()
        
        # --- 3. 更新参数 (Update) ---
        # 调用我们自己写的 sgd 函数
        # 注意：我们在 sgd 内部做了除以 batch_size 的操作
        # 所以这里传进去的参数是用 sum() 算出来的梯度
        sgd([w, b], lr, batch_size)
    
    # --- 4. 监控进度 ---
    # 每个 epoch 结束后，我们用当前训练好的 w, b 在整个数据集上跑一遍
    # 看看平均损失降到了多少。
    with torch.no_grad(): # 评估模式，不需要算梯度
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

