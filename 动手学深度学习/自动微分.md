
我们要对函数 $y = 2\mathbf{x}^{\top}\mathbf{x}$ 求导。
$\mathbf{x}$ 是一个列向量，比如 $\mathbf{x} = [x_0, x_1, x_2, x_3]^{\top}$。
我们先手动推导一下这个函数的梯度，以便后面验证。
公式推导 ($y = 2 * X^T * X$ )：
$\mathbf{x}^{\top}\mathbf{x}$ 是向量 $\mathbf{x}$ 和自身的点积（dot product）。
$y = 2 \times (x_0^2 + x_1^2 + x_2^2 + x_3^2)$
这是一个标量 $y$ 关于向量 $\mathbf{x}$ 的函数。它的梯度 $\nabla_{\mathbf{x}} y$ 也是一个向量，定义为：

$$\nabla_{\mathbf{x}} y = \left[ \frac{\partial y}{\partial x_0}, \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \frac{\partial y}{\partial x_3} \right]^{\top}$$

我们来求 $y$ 对 $\mathbf{x}$ 中任意一个分量 $x_i$ 的偏导数：

$$\frac{\partial y}{\partial x_i} = \frac{\partial}{\partial x_i} (2x_0^2 + 2x_1^2 + \dots + 2x_i^2 + \dots + 2x_3^2)$$

根据求导法则，当 $j \neq i$ 时，$\frac{\partial (2x_j^2)}{\partial x_i} = 0$。只有当 $j = i$ 时，$\frac{\partial (2x_i^2)}{\partial x_i} = 2 \times (2x_i) = 4x_i$。
所以，$\frac{\partial y}{\partial x_i} = 4x_i$。
因此，梯度向量 $\nabla_{\mathbf{x}} y = [4x_0, 4x_1, 4x_2, 4x_3]^{\top} = 4\mathbf{x}$。
**验证**：我们的目标就是让框架自动计算出 $4\mathbf{x}$。
- 步骤1：创建变量
    在所有框架中，我们都创建了 $\mathbf{x} = [0.0, 1.0, 2.0, 3.0]$。
- 步骤2：请求梯度存储
    框架需要知道我们要对哪个变量求导。
    - 在 PyTorch 中，我们通过 `x.requires_grad_(True)` 或 `stop_gradient=False` 来**标记** `x`，告诉框架：“请跟踪所有基于 `x` 的计算，因为我稍后会需要 `x` 的梯度”。`x.grad` 属性此时是 `None`。
- 步骤3：记录计算
    我们需要一个“录音机”来记录计算图。
    - 在 PyTorch (新版) 中，只要 `requires_grad=True`，计算会自动被记录，不需要显式的 `with` 块。
    - 课件中计算了 `y = 2 * torch.dot(x, x)`。
    - 计算结果 $y = 2 \times (0^2 + 1^2 + 2^2 + 3^2) = 2 \times (0 + 1 + 4 + 9) = 2 \times 14 = 28.0$。
- 步骤4：反向传播
    这是最关键的一步。我们告诉框架从最终结果 y 开始反向计算梯度。
    - `y.backward()` (PyTorch / Paddle / MXNet)
    - `x_grad = t.gradient(y, x)` (TensorFlow)
    - 框架会沿着计算图，利用链式法则，计算 $y$ 对所有被标记的叶子节点（这里是 `x`）的梯度。
- **步骤5：获取梯度**
    - 我们检查 `x.grad` (或 TensorFlow 中的 `x_grad`)。
    - 结果应该是$4\mathbf{x} = 4 \times [0.0, 1.0, 2.0, 3.0] = [0.0, 4.0, 8.0, 12.0]$。
    - 课件中的 `x.grad == 4 * x` 验证了这一点，返回 `True`。
- 步骤6：梯度累加
    课件接着计算了另一个函数 $y = \mathbf{x}.\text{sum()}$，即 $y = x_0 + x_1 + x_2 + x_3$。
    它的梯度 $\frac{\partial y}{\partial x_i} = 1$，所以梯度向量应该是 $[1.0, 1.0, 1.0, 1.0]$。
    - **PyTorch：** 它们默认会累加梯度。如果你不清除梯度，直接调用 `y.backward()`，`x.grad` 会变成 `[0., 4., 8., 12.] + [1., 1., 1., 1.] = [1., 5., 9., 13.]`。这在绝大多数情况下都不是我们想要的。
    - **正确的做法**：在PyTorch中，每次计算新梯度之前（通常是在优化循环 `for epoch...` 的开始），必须手动清零：`x.grad.zero_()` 或 `x.clear_gradient()`。
# 非标量变量的反向传播

在深度学习中，我们通常会一次性处理一个小批量 的数据，而不是单个样本。假设批量大小是 $N$，我们的损失函数 $L$ 实际上是一个包含了 $N$ 个样本损失的向量 $\mathbf{l} = [l_1, l_2, \dots, l_N]^{\top}$。

我们真正想优化的，是所有样本的平均损失或总损失，即一个标量：

$$L_{\text{total}} = \sum_{i=1}^N l_i$$

我们需要的是 $L_{\text{total}}$ 对参数 $\mathbf{w}$ 的梯度 $\nabla_{\mathbf{w}} L_{\text{total}}$，而不是 $l_i$ 对 $\mathbf{w}$ 的梯度（这将是一个矩阵，称为雅可比矩阵，计算开销很大）。

根据微积分，梯度的和等于和的梯度：

$$\nabla_{\mathbf{w}} L_{\text{total}} = \nabla_{\mathbf{w}} \left(\sum_{i=1}^N l_i\right) = \sum_{i=1}^N (\nabla_{\mathbf{w}} l_i)$$

课程的例子：

课程中，y = x * x（即 $y = [x_0^2, x_1^2, x_2^2, x_3^2]^{\top}$）就是模拟这个 $\mathbf{l}$ 向量。

当我们直接对一个向量 y 调用 y.backward() 时：

- **PyTorch**：会报错。它们要求你必须显式地将向量 $y$ 转换成一个标量。最常见的做法就是 `y.sum().backward()`。

补充知识：PyTorch 的 `y.backward(torch.ones_like(y))` 其实是计算 $\nabla_{\mathbf{x}} (\mathbf{y}^{\top}\mathbf{v})$，其中 $\mathbf{v}$ 是 `torch.ones_like(y)`。这在数学上等价于 `y.sum().backward()`，但 `y.sum()` 更直观。

# 分离计算

有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算$z = u\times x$ 关于`x`的偏导数，同时将`u`作为常数处理， 而不是$z=x\times x\times x$ 关于`x`的偏导数。

```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
```

`y.detach()` 并没有销毁 `y` 本身。`y` 依然在计算图中。课件的最后一个代码块验证了这一点：它重新对 `y`（不是 `z` 或 `u`）调用 `y.sum().backward()`，`x.grad` 被正确地覆盖（或累加）为 $2\mathbf{x}$（即 `[0., 2., 4., 6.]`）。

# Python控制流的梯度计算

使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
    
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
```

**分析**：
- `while` 循环的次数 $n$ 取决于 $a$ 的初始值。`b` 的最终值是 $a \times 2^{n+1}$。
- `if` 判断的结果也取决于 $a$。
- 但无论如何，最终的输出 $c$ 始终是 $a$ 的一个线性函数：$c = k \times a$，其中 $k$ 是一个（取决于 $a$ 的）常数。
    - 如果 `b.sum() > 0`，则 $c = a \times 2^{n+1}$，所以 $k = 2^{n+1}$。
    - 如果 `b.sum() <= 0`，则 $c = 100 \times a \times 2^{n+1}$，所以 $k = 100 \times 2^{n+1}$
- **推导**：因为 $c = k \times a$（$k$ 在 $a$ 确定的那一刻就是常数），所以 $\frac{dc}{da} = k$。
- 而 $k = \frac{c}{a}$。
- **验证**：因此，框架计算出的梯度 `a.grad` 应该等于 `d / a`（`d` 是 `c` 的最终值）。课件中 `a.grad == d / a` 验证了这一点。
**核心思想**：动态图的自动微分是**即时**发生的。你每执行一步Python代码，计算图就构建一点。`if` 语句执行了哪条路径，计算图就只记录哪条路径；`while` 循环了多少次，计算图就“展开”多少次。它只记录实际发生的计算，所以能完美处理动态控制流。