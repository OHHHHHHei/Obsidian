# 范数与权重衰减

- **基本思想**：我们将函数 $f=0$（即所有权重都为0）视为“最简单”的模型。我们希望模型尽量靠近这个最简单的状态。
- **如何衡量距离？** 使用线性函数 $f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$ 中权重向量 $\mathbf{w}$ 的范数。
- **$L_2$ 正则化**：最常用的方法。我们不仅要最小化预测损失，还要最小化权重的 $L_2$ 范数。

原始损失函数

$$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$$

- **解释**：这是标准的均方误差损失（MSE）。我们在第3章线性回归中学过。
加入惩罚项后的损失函数
$$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2$$
- **详细推导与解释**：
    1. **$L(\mathbf{w}, b)$**：这是上面的原始损失，负责让模型预测得准。
    2. **$\|\mathbf{w}\|^2$**：这是惩罚项。它是向量 $\mathbf{w}$ 中所有元素平方的和（$w_1^2 + w_2^2 + ...$）。
    3. **$\lambda$ (Lambda)**：正则化常数（超参数）。
        - 如果 $\lambda = 0$，就是原始回归，没限制。
        - 如果 $\lambda \to \infty$，为了让总损失变小，$\mathbf{w}$ 必须被迫趋近于0（模型变成一条水平线）。
    4. **为什么除以 2？**：$\frac{\lambda}{2}$ 中的分母 2 是为了数学上的方便。当你对 $\|\mathbf{w}\|^2$ 求导时，平方的导数会产生一个 2，正好跟分母抵消，让梯度公式更干净（变成 $\lambda \mathbf{w}$）。

## **$L_2$ 范数 vs $L_1$ 范数 (Ridge vs Lasso)**
- **$L_2$ (岭回归 Ridge Regression)**：惩罚大的权重。如果你有一个很大的权重 $w_j$，平方后它会变得巨大，惩罚很重。这迫使模型倾向于**均匀分布权重**，让所有权重都比较小，而不是让某几个特别大。这对于抗噪声（鲁棒性）很有好处。
- **$L_1$ (套索回归 Lasso Regression)**：惩罚权重的绝对值。它倾向于让不重要的权重**直接变为0**。这具有**特征选择**（Feature Selection）的功能。
## **梯度更新与“衰减”的由来**
这是必须掌握的推导，解释了为什么叫“权重衰减”。
我们对公式 4.5.2 关于 $\mathbf{w}$ 求梯度。
总梯度的计算如下：

$$\frac{\partial L_{new}}{\partial \mathbf{w}} = \frac{\partial L_{original}}{\partial \mathbf{w}} + \frac{\partial}{\partial \mathbf{w}} (\frac{\lambda}{2} \|\mathbf{w}\|^2)$$
$$\frac{\partial L_{new}}{\partial \mathbf{w}} = \frac{\partial L_{original}}{\partial \mathbf{w}} + \lambda \mathbf{w}$$
现在，代入随机梯度下降（SGD）的更新公式：
$\mathbf{w} \leftarrow \mathbf{w} - \eta \times \text{总梯度}$
（其中 $\eta$ 是学习率）
$$\mathbf{w} \leftarrow \mathbf{w} - \eta \left( \frac{\partial L_{original}}{\partial \mathbf{w}} + \lambda \mathbf{w} \right)$$
展开括号，合并同类项：
$$\mathbf{w} \leftarrow \mathbf{w} - \eta \lambda \mathbf{w} - \eta \frac{\partial L_{original}}{\partial \mathbf{w}}$$

公式 4.5.3：最终更新公式
$$\mathbf{w} \leftarrow (1 - \eta \lambda) \mathbf{w} - \eta \frac{\partial L_{original}}{\partial \mathbf{w}}$$
- **深度解析**：
    - 如果没有正则化（$\lambda=0$），公式就是 $\mathbf{w} \leftarrow \mathbf{w} - \eta \cdot \text{梯度}$。
    - 有了正则化，我们在减去数据梯度之前，先用 $(1 - \eta \lambda)$ 乘了一下 $\mathbf{w}$。
    - 通常 $\eta \lambda$ 是一个很小的正数，所以 $(1 - \eta \lambda) < 1$。
    - **结论**：这意味着，无论数据梯度如何，**权重在每一步更新时都会自动缩小一点点（衰减）**。这就是“权重衰减”名称的数学本质。

# 高维线性回归

