# 初始化模型参数

- **输入层维度推导**：
    - **公式**：$28 \times 28 = 784$。
    - **讲解**：Fashion-MNIST 的图片是灰度图，高和宽都是 28 像素。多层感知机（MLP）通常处理的是“向量”输入，而不是矩阵。因此，我们需要把二维图像“拉平”（Flatten）成一维向量。所以输入特征数 `num_inputs` 固定为 784。这里我们暂时忽略了像素之间的空间结构（这是卷积神经网络CNN要解决的问题，目前先不考虑）。
- **输出层维度推导**：
    - **讲解**：因为数据集有 10 个类（T恤、裤子等），所以输出层必须有 10 个神经元，每个神经元代表属于某一类的“得分”。`num_outputs` 固定为 10。
- **隐藏层维度设定（超参数）**：
    - **设定**：课件中选择了一个**单隐藏层**，包含 256 个隐藏单元（`num_hiddens = 256`）。
    - **为什么要这样做？**：
        1. 隐藏层必须比输入层有足够的“容量”来学习特征，但也比输出层大，以便在压缩信息前提取足够多的模式。
        2. **重点**：为什么选 256？这涉及到底层硬件设计。计算机内存分配和寻址通常是按 2 的幂次进行的（如 32, 64, 128, 256）。选择 2 的幂次作为层宽，在矩阵运算时往往能获得更高的计算效率。这在工程上是一个通过经验得出的“最佳实践”。
- 参数张量的形状（Shape）推导：
    我们要初始化的参数有四个：$W_1, b_1$（第一层）和 $W_2, b_2$（第二层）。
    - **第一层（输入 $\to$ 隐藏）**：
        - 输入 $X$ 的形状是 `(batch_size, 784)`。
        - 我们要得到隐藏层输出 $H$，其形状应该是 `(batch_size, 256)`。
        - 根据矩阵乘法公式 $H = XW_1 + b_1$：
            - $W_1$ 的形状必须是 **(784, 256)**。
            - $b_1$ 的形状是 **(256)** （广播机制会使其对每个样本生效）
        - **代码对应**：
            - PyTorch: `W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)`
            - 这里乘以 `0.01` 是为了让初始权重数值很小，呈正态分布。如果权重过大，可能会导致后续激活函数梯度消失或爆炸（虽然ReLU对梯度消失抵抗较好，但保持小权重是好习惯）。
    - **第二层（隐藏 $\to$ 输出）**：
        - 现在的输入是 $H$，形状 `(batch_size, 256)`。
        - 我们要得到输出 $O$，形状 `(batch_size, 10)`。
        - 根据公式 $O = HW_2 + b_2$：
            - $W_2$ 的形状必须是 **(256, 10)**。
            - $b_2$ 的形状是 **(10)**。
        - **代码对应**：
            - PyTorch: `W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)`
    - **关于 `requires_grad=True`**：
        - **讲解**：一定要告诉框架，这些变量是“参数”，我们需要在反向传播中计算它们对于损失函数的梯度，以便更新它们。这是“从零开始”中最容易忘记的一步。
```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

# 激活函数
```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

# 模型
```python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```
- **一：数据展平（Flatten）**
    - **代码**：`X = X.reshape((-1, num_inputs))`
        - 我们的原始输入是图片批次，形状为 `(batch_size, 28, 28)`。
        - 但是全连接层（矩阵乘法）需要输入是二维矩阵 `(样本数, 特征数)`。
        - 因此，必须先用 `reshape` 将每张 $28 \times 28$ 的图片拉直成长度为 784 的向量。
        - `-1` 是由框架自动推断的，代表 `batch_size`。
- **步骤二：隐藏层计算**
    - **公式**：$H = \text{ReLU}(X W_1 + b_1)$
    - **代码**：`H = relu(np.dot(X, W1) + b1)` （以MXNet/NumPy风格为例，PyTorch使用 `@` 符号表示矩阵乘法
    - **解释**：
        1. `np.dot(X, W1)`：线性变换，形状变为 `(batch_size, 256)`。
        2. `+ b1`：广播机制，给每个样本加上偏置。
        3. `relu(...)`：**关键一步**，引入非线性。如果没有这一步，这就是个线性变换。
- **步骤三：输出层计算**
    - **公式**：$O = H W_2 + b_2$
    - **代码**：`return np.dot(H, W2) + b2`
    - **解释**
        - 将隐藏层的输出 $H$ 作为输出层的输入。
        - 形状变化：`(batch_size, 256)` $\times$ `(256, 10)` $\to$ `(batch_size, 10)`。
        - **注意**：输出层通常**不再加激活函数**（或者说，Softmax 操作通常留给后面的损失函数去处理，这里只输出原始得分 Logits）。
# 损失函数

```python
loss = nn.CrossEntropyLoss(reduction='none')
```
# 训练

```python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```