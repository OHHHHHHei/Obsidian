# 为何抛弃MLP？

假设输入图片是一张普通的 1000 $\times$ 1000 像素的高清图，输入维度 $d = 1,000,000$（一百万）。
如果我们用上一章学的多层感知机（MLP），第一层隐藏层只设 1000 个神经元。
那么，权重矩阵 $W$ 的大小就是 $1000 \times 1,000,000 = 10^9$（十亿）。
**十亿个参数！** 光存储这些 float32 数据就需要 4GB 显存，而且这还只是一层！这在计算上是不现实的，且极易过拟合。所以，全连接层（Dense Layer）不适合处理高维感知的图像数据。
# 不变性

为了解决这个问题，我们必须利用图像特有的性质来简化模型。课件里用了“沃尔多在哪里”（Where's Waldo）这个游戏做比喻。
##  **平移不变性 (Translation Invariance)**：

- **解释**：不管沃尔多躲在图片的左上角还是右下角，他都是沃尔多。
- **自动化类比**：流水线上流过来的零件，不管它在传送带的左边还是右边，你的算法都应该能识别出它是同一个零件。检测器（权重）不应该随位置变化。
## **局部性 (Locality)**：

- **解释**：要判断这是不是沃尔多的眼睛，我们只需要看眼睛周围的像素。我们不需要把眼睛的像素和图片最远角的像素联系起来。
- **自动化类比**：在做边缘检测时，判断一个点是不是边缘，只取决于它和周围邻居的灰度差，和距离它一米远的点无关。
# 多层感知机的限制

假设输入图像 $X$ 是二维的（大小 $h \times w$），隐藏层输出 $H$ 也是二维的。
为了让 MLP 能处理二维图像，我们把 $W$ 扩展成一个四阶张量。
全连接层公式如下：
$$[H]_{i,j} = [U]_{i,j} + \sum_k \sum_l [W]_{i,j,k,l} [X]_{k,l}$$
- $[H]_{i,j}$：输出图在坐标 $(i,j)$ 的值。
- $[X]_{k,l}$：输入图在坐标 $(k,l)$ 的像素值。
- $[W]_{i,j,k,l}$：**权重**。意思是：输入位置 $(k,l)$ 的像素，对输出位置 $(i,j)$ 的影响权重。每一个输入像素都影响每一个输出像素，这就是“全连接”。
## 应用平移不变性

**思想**：检测器不应该随位置 $(i,j)$ 变化。也就是说，权重的性质只取决于输入点 $(k,l)$ 和输出点 $(i,j)$ 的**相对距离**，而不是绝对坐标。
我们可以令 $a = k-i, b = l-j$（相对位移）。
那么权重 $W$ 不再依赖于 $i, j$，只依赖于相对位移 $a, b$。我们把 $W$ 重命名为 $V$：
$$[W]_{i,j,k,l} = [V]_{i-k, j-l} = [V]_{a,b}$$
同时，偏置 $U$ 也就是个常数 $u$ 了。
代入公式，得到：
$$[H]_{i,j} = u + \sum_a \sum_b [V]_{a,b} [X]_{i+a, j+b}$$
- **变化**：现在 $V$ 的参数量大大减少了，它不再跟图像尺寸有关，只跟相对位移的范围有关。这就实现了**权重共享**。
## 应用局部性

**思想**：输出点 $(i,j)$ 只应该受周围像素的影响。如果输入点距离太远（$|a| > \Delta$ 或 $|b| > \Delta$），权重应该为 0。
这意味着，我们只需要在一个小范围 $\Delta$（比如 $3 \times 3$ 或 $5 \times 5$）内求和，而不是对整张图求和。
$$[H]_{i,j} = u + \sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} [V]_{a,b} [X]_{i+a, j+b}$$
这就是**卷积层（Convolutional Layer）** 的定义！
- 这里的权重 $V$（大小为 $2\Delta+1 \times 2\Delta+1$），就是我们常说的**卷积核（Kernel）**或**滤波器（Filter）**。
- 参数量从“十亿”级别降到了“几十”（例如 $3 \times 3 = 9$ 个参数）。
# 卷积

## 连续形式
$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}$$
- **解释**：这里 $f$ 是输入信号，$g$ 是卷积核（或者叫冲激响应）。
- **关键点**：注意 $g(\mathbf{x}-\mathbf{z})$ 中的**减号**。这意味着在积分之前，函数 $g$ 需要先关于原点做**翻转（Flip）**，然后平移 $\mathbf{x}$，最后再和 $f$ 相乘积分。
## 离散形式

因为计算机处理的是离散数据，积分变成了求和：
$$(f * g)(i) = \sum_a f(a) g(i-a)$$
- **解释**：这依然保留了 $i-a$ 的形式，意味着**翻转**依然存在。
## 二维离散形式

到了图像处理（二维信号），公式变成了双重求和：
$$(f * g)(i, j) = \sum_a \sum_b f(a, b) g(i-a, j-b)$$
- **解释**：$f$ 是图像，$g$ 是卷积核。注意索引是 $(i-a, j-b)$。
- **直观理解**：在数学标准定义下，你需要把卷积核上下颠倒、左右翻转，然后再去和图像做乘法累加。
## 深度学习中的“卷积”（其实是互相关）

现在，请回头看我们推导出的公式：
$$[H]_{i,j} = \sum_a \sum_b [V]_{a,b} [X]_{i+a, j+b}$$
- **对比发现**：
    - 数学公式用的是 **减号** $(i-a, j-b)$（翻转）。
    - 深度学习公式用的是 **加号** $(i+a, j+b)$（不翻转）。
- **结论**：
    - 严格来说，深度学习中使用的运算在数学上被称为 **互相关（Cross-correlation）**。
    - 互相关和卷积的区别仅仅在于**是否对核进行了翻转**。
# 通道

之前的推导假设图片是黑白的（灰度图），每个像素只有一个值（亮度）。
但生活中的图片是彩色的，由 R（红）、G（绿）、B（蓝）三原色组成。
这意味着，位于坐标 $(i, j)$ 的像素，不再是一个标量，而是一个长度为 3 的向量 $[R, G, B]$。
所以，输入数据 $X$ 的形状从 $1024 \times 1024$ 变成了 $1024 \times 1024 \times 3$。
这就引入了第三个维度——通道（Channel）。
## 权重的升级：从二维到四维

既然输入变厚了（有 3 层），我们的“探测器”（卷积核）也不能只是薄薄的一张纸了，它也得有厚度，必须穿透这 3 层。

1. **处理输入通道（Input Channels）**：
    - 如果输入有 3 个通道，我们的卷积核 $V$ 也必须有 3 个通道。
    - 比如，卷积核大小是 $3 \times 3$，那么实际上它是 $3 \times 3 \times 3$ 的立体块。
    - 计算时，卷积核的第 1 层跟图片的 R 通道卷，第 2 层跟 G 通道卷，第 3 层跟 B 通道卷，然后把这三个结果**加起来**（$\sum_c$），得到一个数值。
    - **注意**：不管输入有多少通道，**一个**卷积核卷完之后，输出的永远是**单通道**（一张皮）。
2. **生成输出通道（Output Channels）**：
    - 只有一张特征图是不够的。我们可能需要一张图来表示“哪里有垂直边缘”，另一张图表示“哪里是红色”，再一张图表示“哪里有纹理”。
    - 每一张特征图都需要**一个独立的卷积核**来生成。
    - 如果我们想要输出 $d$ 个通道（即 $d$ 种特征），我们就需要准备 $d$ 个不同的卷积核。
    - 于是，权重 $V$ 的维度就变成了：
        $$[V]_{a, b, c, d}$$
        - $a, b$：空间大小（如 $3 \times 3$）
        - $c$：输入通道数（匹配输入 $X$）
        - $d$：输出通道数（决定输出 $H$ 的厚度）
## 三、 通用卷积公式

让我们逐项拆解图片中的核心公式：
$$[H]_{i,j,d} = \sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_{c} [V]_{a,b,c,d} [X]_{i+a, j+b, c}$$
- **$[H]_{i,j,d}$**：这是输出（隐藏层）。
    - $i, j$：输出的空间坐标。
    - $d$：第 $d$ 个输出通道（第 $d$ 种特征）。
- **$\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta}$**：这是**空间上的卷积**
    - $a, b$：在卷积核的空间窗口内滑动（比如 $3 \times 3$ 范围）。
- **$\sum_{c}$**：这是**通道上的融合**。
    - **关键点**：这里对输入的所有通道 $c$ 进行了求和。这意味着输入的多通道信息被“压缩”或“融合”成了一个值。
- **$[V]_{a,b,c,d}$**：这是权重（卷积核）。
    - 它负责把输入通道 $c$ 的信息，通过空间位置 $a, b$，转换到输出通道 $d$ 上。
- **$[X]_{i+a, j+b, c}$**：这是输入。
    - 取的是坐标 $(i+a, j+b)$ 处，第 $c$ 个通道的像素值。
总结一句话：

计算输出的第 $d$ 层特征图时，我们用第 $d$ 个卷积核（立体块），在输入的 $c$ 个通道上同时滑动，把空间邻域和所有通道的信息加权求和，汇聚成一个点。
