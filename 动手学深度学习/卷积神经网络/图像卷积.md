# 互相关运算

![[Pasted image 20251207000240.png]]
## 卷积”与“互相关”的辨析
严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算。
- **数学背景**：在《信号与系统》中，卷积 $f(t)*g(t)$ 要求其中一个函数先**翻转**（反褶）再平移。
- **深度学习中**：我们直接平移卷积核，不做翻转。
- **为什么这样做**：因为卷积核的参数是**学出来的**（Training）。如果我们需要翻转的效果，模型自己会学习出“翻转后”的参数矩阵。因此，为了计算方便，我们直接省略翻转步骤，统称为“卷积”。
## 二维互相关运算的图解

请看文档中的 图6.2.1。这是本节最经典的例子
- **输入 (Input)**: 一个 $3 \times 3$ 的二维张量（代表图像）。$$ \begin{bmatrix} 0 & 1 & 2 \\ 3 & 4 & 5 \\ 6 & 7 & 8 \end{bmatrix}$$
- **卷积核 (Kernel)**: 一个 $2 \times 2$ 的张量（代表滤波器）。$$ \begin{bmatrix} 0 & 1 \\ 2 & 3 \end{bmatrix}$$
- 运算过程（滑动窗口）：
卷积核从输入的左上角开始，从左到右、从上到下滑动。
- 第一步（计算输出的左上角元素）：
卷积核覆盖输入的左上角区域（阴影部分）：$$\text{输入切片} = \begin{bmatrix} 0 & 1 \\ 3 & 4 \end{bmatrix}$$操作是按元素相乘（Element-wise multiplication）然后求和：$$0\times0 + 1\times1 + 3\times2 + 4\times3 = 0 + 1 + 6 + 12 = 19$$所以，输出矩阵的 $(0,0)$ 位置是 19。
- 第二步（向右滑动一格，计算 25）：
卷积核右移，覆盖输入的第一排第2、3列和第二排第2、3列：$$\text{输入切片} = \begin{bmatrix} 1 & 2 \\ 4 & 5 \end{bmatrix}$$
计算：
$$1\times0 + 2\times1 + 4\times2 + 5\times3 = 0 + 2 + 8 + 15 = 25$$
- 第三步（换行，计算 37）：
卷积核回到左侧，但在垂直方向下移一格：
$$\text{输入切片} = \begin{bmatrix} 3 & 4 \\ 6 & 7 \end{bmatrix}$$
计算：
$$3\times0 + 4\times1 + 6\times2 + 7\times3 = 0 + 4 + 12 + 21 = 37$$

- 第四步（右移，计算 43）：
$$\text{输入切片} = \begin{bmatrix} 4 & 5 \\ 7 & 8 \end{bmatrix}$$
计算：
$$4\times0 + 5\times1 + 7\times2 + 8\times3 = 0 + 5 + 14 + 24 = 43$$
- **最终输出**：

$$ \begin{bmatrix} 19 & 25 \\ 37 & 43 \end{bmatrix}$$

## 输出大小的推导公式
文档中给出了公式 (6.2.2)。
假设：
- 输入大小：$n_h \times n_w$ （高度 $n_h$，宽度 $n_w$）
- 卷积核大小：$k_h \times k_w$
输出的大小是多少？
$$(n_h - k_h + 1) \times (n_w - k_w + 1)$$
详细推导解释（为什么是这个公式？）：
想象一个长度为 $n$ 的条带，我们要放一个长度为 $k$ 的尺子上去滑动。
1. 尺子的左端点从条带的索引 $0$ 开始。
2. 尺子的右端点最远能到达条带的索引 $n$。
3. 当尺子左端点在位置 $i$ 时，尺子占据 $[i, i+k]$。
4. 为了保证尺子不超出条带，左端点的最大索引是 $n - k$。
5. 从索引 $0$ 到 $n-k$，总共有 $(n-k) - 0 + 1$ 个可能的位置。
所以，高度方向输出 $n_h - k_h + 1$ 行，宽度方向输出 $n_w - k_w + 1$ 列。
在上面的例子中：
$n_h=3, k_h=2 \Rightarrow 3-2+1 = 2$
$n_w=3, k_w=2 \Rightarrow 3-2+1 = 2$
输出确实是 $2 \times 2$。
## 代码实现
```Python
import torch

def corr2d(X, K):  # X是输入，K是卷积核
    """计算二维互相关运算"""
    # 1. 获取卷积核的形状 (h, w)
    # 对应数学中的 k_h, k_w
    h, w = K.shape 

    # 2. 初始化输出张量 Y
    # 根据公式：(n_h - k_h + 1) x (n_w - k_w + 1)
    # X.shape[0] 是输入高度，X.shape[1] 是输入宽度
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))

    # 3. 双重循环模拟滑动窗口
    # 外层循环遍历输出的每一行
    for i in range(Y.shape[0]):
        # 内层循环遍历输出的每一列
        for j in range(Y.shape[1]):
            # 4. 核心计算
            # X[i : i + h, j : j + w] 就是当前窗口覆盖的输入区域
            # * K 表示按元素乘法 (Element-wise multiplication)
            # .sum() 表示将乘积结果求和
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
            
    return Y
```
# 卷积层

## **从函数到层**

之前写的 `corr2d` 只是一个计算函数。为了让它成为神经网络的一层，我们需要把卷积核 $K$ 和偏置 $b$ 变成**可训练的参数（Parameters）**。
## 定义

我们在文档中看到 `Conv2D` 类的实现
```Python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        # 1. 声明权重参数 weight
        # nn.Parameter 告诉 PyTorch 这是一个需要计算梯度的模型参数
        self.weight = nn.Parameter(torch.rand(kernel_size))
        # 2. 声明偏置参数 bias
        # bias 通常初始化为 0
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        # 3. 前向传播：互相关运算 + 偏置
        return corr2d(x, self.weight) + self.bias
```
- **详细解释**：
    - **`kernel_size`**：决定了权重的形状（比如 $1\times2$ 或 $3\times3$）。
    - **`corr2d(x, self.weight) + self.bias`**：
        - 这里有一个隐式的**广播机制（Broadcasting）**。`corr2d` 输出的是一个矩阵（比如 $6 \times 7$），而 `bias` 是一个标量。
        - 相加时，这个标量会被加到矩阵的**每一个元素**上。这相当于给整个特征图加了一个直流分量（DC offset）。


# 图像中目标的边缘检测

这是卷积神经网络最直观的解释。我们要检测图像中物体轮廓的边缘。
- 构造图像 $X$：
    文档构造了一个 $6 \times 8$ 的图像。
    - 左边两列是 1（白色）。
    - 中间四列是 0（黑色）。
    - 右边两列是 1（白色）。
    - 这就形成了一个“白-黑-白”的图案，也就意味着有两条**垂直边缘**。$$X = \begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\ ... & ... & ... & ... & ... & ... & ... & ... \\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \end{bmatrix}$$
- 设计卷积核 $K$：
    构造一个 $1 \times 2$ 的卷积核：
    $$K = \begin{bmatrix} 1 & -1 \end{bmatrix}$$
- 数学推导（为什么这个核能检测边缘？）
    当我们将 $K$ 在 $X$ 上滑动时，实际上是在做相邻两个像素的减法（当前像素 - 右边像素）。
    - 情况 A：平坦区域（纯白或纯黑）
        如果窗口覆盖的是 $[1, 1]$ 或 $[0, 0]$：$$1\times1 + 1\times(-1) = 0$$$$0\times1 + 0\times(-1) = 0$$
        结果为 0，表示没有变化，不是边缘。
    - 情况 B：从白到黑的边缘（下降沿）
        窗口覆盖 $[1, 0]$：$$1\times1 + 0\times(-1) = 1$$
        结果为 1，检测到边缘（正值表示由亮变暗）。
    - 情况 C：从黑到白的边缘（上升沿）
        窗口覆盖 $[0, 1]$：$$0\times1 + 1\times(-1) = -1$$
        结果为 -1，检测到边缘（负值表示由暗变亮）。
- 输出结果 $Y$：
    文档中的输出 $Y$ 完美印证了上述推导：$$Y = \begin{bmatrix} 0 & 1 & 0 & 0 & 0 & -1 & 0 \\ 0 & 1 & 0 & 0 & 0 & -1 & 0 \\ ... & ... & ... & ... & ... & ... & ... \end{bmatrix}$$
    - 第 2 列全是 1：检测到了左边的垂直边缘。
    - 第 6 列全是 -1：检测到了右边的垂直边缘。
    - 其他全是 0：平坦区域。
## 方向性限制

- **实验**：将图像 $X$ 转置（变成横向条纹），再用同一个核 $K=[1, -1]$ 去卷。
- **结果**：输出全为 0。
- **原因**：
    - $X^T$ 的变化发生在垂直方向（上下行之间）。
    - 核 $K=[1, -1]$ 是在水平方向（左右列之间）做差分。
    - 在水平方向上，$X^T$ 的每一行数值都是一样的（比如全是1或全是0），没有变化。
    - **结论**：特定的卷积核只能提取特定的特征（这里是垂直边缘）。如果想检测水平边缘，需要用 $K^T = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$。
# 学习卷积核

- **问题提出**：对于复杂的图像特征（如猫的耳朵、汽车的轮子），我们不可能像边缘检测那样手动设计出成千上万个完美的卷积核 $K$。
- **解决方案**：我们提供输入 $X$ 和期望的输出 $Y$（标签），让计算机自动寻找最佳的 $K$。这通过最小化预测值与真实值的**均方误差**来实现。

我们已知输入 $X$ 和由 $[1, -1]$ 核生成的输出 $Y$。现在假设我们不知道核是 $[1, -1]$，而是随机初始化一个 $1 \times 2$ 的核，看看能不能把 $[1, -1]$ 学回来。

- **实验步骤**：
    1. **构造**：定义一个卷积层 `Conv2D(1, kernel_size=(1, 2))`，忽略偏置。
    2. **初始化**：权重 `weight` 被随机初始化（比如 $[0.9, -0.2]$，完全不对）。
    3. **训练循环（迭代 10 次）**：
        - **前向传播**：用当前的随机核去卷 $X$，得到预测值 $\hat{Y}$。
        - **计算损失**：计算 $\hat{Y}$ 和真实 $Y$ 的差距（均方误差 $l = (\hat{Y} - Y)^2$）。
        - **反向传播**：计算梯度 $\nabla l$，告诉权重“该往哪个方向变”。
        - **更新参数**：利用梯度下降法修改权重。
    4. **结果**：经过 10 次迭代，Loss 降到了 0.004，打印出来的权重变成了 `[0.9895, -0.9873]`。
- **结论**：这非常接近真实的 $[1, -1]$ 。这意味着，只要有足够的数据，神经网络完全可以**自动学会**提取边缘（或者更复杂的纹理）所需的算子，而不需要人类干预。
# 互相关 vs 卷积

- **严格卷积**：核需要水平翻转 + 垂直翻转。
- **深度学习**：不翻转。
- **为什么没关系？**：因为核是学出来的。
    - 如果任务需要一个“左上角正，右下角负”的特征提取器。
    - 严格卷积会学出一个“右下角正，左上角负”的核（因为计算时会翻转回来）。
    - 深度学习会直接学出“左上角正，右下角负”的核。
    - **最终效果在数学上是等价的**。
# 特征图与感受野

请回看模块一的计算图。
- 特征图 (Feature Map)：
    卷积层的输出 $Y$ 被称为特征图。因为它不再是原始像素亮度，而是“是否存在某种特征”的映射。
    - 例如：在边缘检测例子中，输出图中的非0值位置，就是原图中存在“垂直边缘特征”的位置。
- **感受野 (Receptive Field) 的传递性**：
    - **第一层**：输出 $Y$ 中的一个元素（比如 $19$），只依赖于输入 $X$ 中的 $2 \times 2$ 区域。它的感受野是 $2 \times 2$。
    - **第二层**：如果我们再把 $Y$ 卷一次得到 $Z$。$Z$ 中的一个元素依赖于 $Y$ 的 $2 \times 2$。
    - **级联效应**：$Y$ 的每个元素又依赖于 $X$ 的 $2 \times 2$。叠加起来，$Z$ 中的一个元素实际上能“看到” $X$ 中更大的区域（比如 $3 \times 3$）。