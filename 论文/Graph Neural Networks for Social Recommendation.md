
# 研究背景

传统推荐系统依赖“用户-物品”交互来进行推荐，忽略了社交关系

## 当前社交推荐模型的痛点

**复杂的图结构：

- 用户其实同时处于两张“图”中：一张是**社交图**（用户关注用户），另一张是**交互图**（用户给物品打分）。
    
- **痛点**：以前的方法很难将这两张图的信息“完美融合”起来

**交互不仅仅是连接，还有“观点” ：

- 在很多图模型里，连线只代表“有过交互”。但实际上，你给某商品打 5 分（喜欢）和打 1 分（讨厌），虽然都是“交互”，含义却完全相反。
    
- **痛点**：如何把“打分的高低”（观点/Opinion）也融入到图的学习中？

**社交关系有强弱之分：

- 你的微信里有几百个好友，但你可能只听几个死党的话。
    
- **痛点**：以前很多模型把所有社交好友一视同仁（平均权重），这显然不合理。我们需要区分“强关系”和“弱关系” 。

## 解决方案

GNN + Attention

**为什么要用 GNN？** 图神经网络天生擅长处理节点和结构信息，特别适合从邻居节点聚合信息 。

**GraphRec 的核心思路**：利用 GNN 的聚合能力，加上 **注意力机制，让模型自动学习：
1. 哪些朋友更重要（社交强度）。
2. 哪些历史交互更能代表用户喜好（兴趣强度）。

# 用户建模

## 物品聚合 (Item Aggregation)：从“历史行为”看用户

### 怎么把“物品+评分”变成一个向量？

$$x_{ia} = g_v([q_a \oplus e_r])$$

**逐个符号解释**：
- $x_{ia}$：**观点感知交互向量。** 这是我们要算出来的，代表“用户 $u_i$ 对物品 $v_a$ 是一次什么样的交互（比如满意的交互 vs 失望的交互）”。
- $q_a$：物品 $v_a$ 的 ID Embedding 。
- $e_r$：评分 $r$ 的 **观点 Embedding** (Opinion Embedding) 3。这是重点！比如 $r=5$，模型会去查表得到一个向量 $e_5$。
- $\oplus$：**拼接操作 (Concatenation)**。把物品向量和观点向量首尾相连。
- $g_v$：**MLP (多层感知机)**。一个神经网络，用来融合物品信息和情感信息。

### 怎么把所有交互历史聚合起来？

现在用户 $u_i$ 有一堆交互历史 $\{x_{ia}\}$（其中 $a \in C(i)$，即所有交互过的物品集合）。我们需要把这一堆向量合成一个向量 $h_i^I$。

**方案 A：平均法**

$$h_i^I = \sigma(W \cdot \{ \sum_{a \in C(i)} \frac{1}{|C(i)|} x_{ia} \} + b)$$

- $\sigma$ 
    - **定义**：**非线性激活函数（Non-linear Activation Function）**。
    - **含义**：论文中具体使用的是 **ReLU** (Rectified Linear Unit)。它的作用是给神经网络引入非线性能力，让模型能拟合更复杂的关系。
- $\mathbf{W}$
    - **定义**：**权重矩阵（Weight Matrix）**。
    - **含义**：这是模型需要通过训练学出来的参数。它的作用是对聚合后的向量进行线性变换（特征提取和维度调整）。
- $\{ ... \}$
    - **含义**：大括号里面包裹的内容，就是 **Aggregator（聚合器）** 的核心逻辑，即把一堆历史记录“揉”在一起的过程。
- $\sum_{a \in C(i)}$
    
    - **定义**：**求和符号**，遍历集合 $C(i)$。
    - $C(i)$：用户 $u_i$ 交互过的**物品集合**（或者说 $u_i$ 在 User-Item 图中的邻居节点。
    - $a$：代表集合中的某一个物品 $v_a$。
    - **含义**：这就表示我们要把用户 $u_i$ 买过的所有东西一个一个拿出来处理。
- $\alpha_i$
    - **定义**：**聚合权重（Aggregation Weight）**。
    - **含义**：它决定了“某次交互”对“用户画像”的贡献有多大。
    - **注意:** 这里的 $\alpha_i$ 通常被设定为 **固定值** $\frac{1}{|C(i)|}$ 。
        - 这意味着这是 **平均聚合** ：假设用户买过的所有东西同等重要
        - _后续进化_：论文在公式 (4) 中认为这样不够好，于是将其改进为 **Attention 权重** $\alpha_{ia}$，让模型自己学哪个物品更重要。
- $\mathbf{x}_{ia}$
    - **定义**：**Opinion-aware Interaction Representation（感知意见的交互表示）**。
    - **含义**：这是一个向量，它不仅仅是物品 $v_a$ 的 ID，而是 **“物品 $v_a$ 的向量”** 加上 **“用户打分 $r$ 的向量”** 经过 MLP 融合后的结果（即前文提到的公式 2）。它代表了那一次具体的交互事件。
- $\mathbf{b}$
    - **定义**：**偏置向量（Bias Vector）** 8。
    - **含义**：神经网络的标准配置，也是可训练的参数，用于调整激活函数的阈值位置。
- 这就相当于把所有历史记录取个平均值。
- **缺点：** 它假设你买的所有东西对你的兴趣定义同等重要。但实际上，你可能随手买了一包纸巾（不重要），和你买了一台高性能游戏本（很重要），这两者对你兴趣画像的贡献是不一样的 5。

**方案 B：注意力机制（Attention-based）—— 论文采用的方法** 

为了解决“平均法”的缺陷，作者引入了 Attention，让模型自己学每个交互的权重。

$$h_i^I = \sigma(W \cdot \{ \sum_{a \in C(i)} \alpha_{ia} x_{ia} \} + b)$$

- 这里 $\alpha_{ia}$ 就是**注意力权重（Attention Weight）**。它代表了物品 $v_a$ 的那次交互对定义用户 $u_i$ 有多重要。
### 权重 $\alpha_{ia}$ 怎么算？

作者用了一个双层神经网络（Attention Network）来计算。

$$x_{ia}^* = w_2^T \cdot \sigma(W_1 \cdot [x_{ia} \oplus p_i] + b_1) + b_2$$

$$\alpha_{ia} = \frac{\exp(x_{ia}^*)}{\sum_{a' \in C(i)} \exp(x_{ia'}^*)}$$

- **解读：**
- 输入：$[x_{ia} \oplus p_i]$。为什么要拼上 $p_i$（用户自身的基础 Embedding）？
	- 因为**同一个物品对不同用户的重要性不同**。必须把“当前物品交互 $x_{ia}$” 和 “目标用户 $p_i$” 放在一起看，才能算出这个物品对**这个**用户的重要程度。
 $W_1, w_2$：这是网络要学习的参数。
 Softmax：把算出来的分数 $x_{ia}^*$ 归一化，保证所有交互的权重加起来等于1。

## Social Aggregation（社交聚合）从朋友圈中刻画用户

### 聚合什么信息？

我们要把用户 $u_i$ 的朋友们（$o \in N(i)$）聚合起来。

注意： 这里聚合的不是朋友的基础 Embedding $p_o$，而是朋友的 Item-space 潜向量 $h_o^I$。
- **为什么？** 因为我们需要朋友的“特征”，而朋友的特征通过他自己的购买历史（$h_o^I$）体现得最淋漓尽致。
### 社交注意力机制（Social Attention）

同样，朋友的影响力有大有小（强关系 vs 弱关系），不能用平均值。所以也用了 Attention。

$$h_i^S = \sigma(W \cdot \{ \sum_{o \in N(i)} \beta_{io} h_o^I \} + b)$$

- $h_i^S$：用户 $u_i$ 的 **Social-space User Latent Factor**。
- $\beta_{io}$：朋友 $u_o$ 对用户 $u_i$ 的影响力权重。

$$\beta_{io}^* = w_2^T \cdot \sigma(W_1 \cdot [h_o^I \oplus p_i] + b_1) + b_2$$

- **解读：** 同样是把“朋友的特征 $h_o^I$”和“我自己的特征 $p_i$”拼起来，输入神经网络，计算出一个分数。这让模型能够模拟现实中的**强弱关系**。

## 合并

现在我们手里有了两块拼图：
1. $h_i^I$：基于行为历史的我。
2. $h_i^S$：基于社交关系的我。
把它们结合起来，得到最终的用户表示 $h_i$。

$$c_1 = [h_i^I \oplus h_i^S]$$

$$h_i = \text{MLP}(c_1)$$

(注：论文中是用多层神经网络表示的，公式13-14就是标准的MLP前向传播过程)
- **物理意义：** 这就是一个特征融合的过程。模型最终学到的 $h_i$ 既包含了用户的个人口味，也包含了社交圈对他的潜移默化影响 16。


# 物品建模

