
# 研究背景

传统推荐系统依赖“用户-物品”交互来进行推荐，忽略了社交关系

## 当前社交推荐模型的痛点

**复杂的图结构：

- 用户其实同时处于两张“图”中：一张是**社交图**（用户关注用户），另一张是**交互图**（用户给物品打分）。
    
- **痛点**：以前的方法很难将这两张图的信息“完美融合”起来

**交互不仅仅是连接，还有“观点” ：

- 在很多图模型里，连线只代表“有过交互”。但实际上，你给某商品打 5 分（喜欢）和打 1 分（讨厌），虽然都是“交互”，含义却完全相反。
    
- **痛点**：如何把“打分的高低”（观点/Opinion）也融入到图的学习中？

**社交关系有强弱之分：

- 你的微信里有几百个好友，但你可能只听几个死党的话。
    
- **痛点**：以前很多模型把所有社交好友一视同仁（平均权重），这显然不合理。我们需要区分“强关系”和“弱关系” 。

## 解决方案

GNN + Attention

**为什么要用 GNN？** 图神经网络天生擅长处理节点和结构信息，特别适合从邻居节点聚合信息 。

**GraphRec 的核心思路**：利用 GNN 的聚合能力，加上 **注意力机制，让模型自动学习：
1. 哪些朋友更重要（社交强度）。
2. 哪些历史交互更能代表用户喜好（兴趣强度）。

# 用户建模

## 物品聚合 (Item Aggregation)：从“历史行为”看用户

### 怎么把“物品+评分”变成一个向量？

$$x_{ia} = g_v([q_a \oplus e_r])$$

**逐个符号解释**：
- $x_{ia}$：**观点感知交互向量。** 这是我们要算出来的，代表“用户 $u_i$ 对物品 $v_a$ 是一次什么样的交互（比如满意的交互 vs 失望的交互）”。
- $q_a$：物品 $v_a$ 的 ID Embedding 。
- $e_r$：评分 $r$ 的 **观点 Embedding** (Opinion Embedding) 3。这是重点！比如 $r=5$，模型会去查表得到一个向量 $e_5$。
- $\oplus$：**拼接操作 (Concatenation)**。把物品向量和观点向量首尾相连。
- $g_v$：**MLP (多层感知机)**。一个神经网络，用来融合物品信息和情感信息。

### 怎么把所有交互历史聚合起来？

现在用户 $u_i$ 有一堆交互历史 $\{x_{ia}\}$（其中 $a \in C(i)$，即所有交互过的物品集合）。我们需要把这一堆向量合成一个向量 $h_i^I$。

**方案 A：平均法**

$$h_i^I = \sigma(W \cdot \{ \sum_{a \in C(i)} \frac{1}{|C(i)|} x_{ia} \} + b)$$

- $\sigma$ 
    - **定义**：**非线性激活函数（Non-linear Activation Function）**。
    - **含义**：论文中具体使用的是 **ReLU** (Rectified Linear Unit)。它的作用是给神经网络引入非线性能力，让模型能拟合更复杂的关系。
- $\mathbf{W}$
    - **定义**：**权重矩阵（Weight Matrix）**。
    - **含义**：这是模型需要通过训练学出来的参数。它的作用是对聚合后的向量进行线性变换（特征提取和维度调整）。
- $\{ ... \}$
    - **含义**：大括号里面包裹的内容，就是 **Aggregator（聚合器）** 的核心逻辑，即把一堆历史记录“揉”在一起的过程。
- $\sum_{a \in C(i)}$
    
    - **定义**：**求和符号**，遍历集合 $C(i)$。
    - $C(i)$：用户 $u_i$ 交互过的**物品集合**（或者说 $u_i$ 在 User-Item 图中的邻居节点。
    - $a$：代表集合中的某一个物品 $v_a$。
    - **含义**：这就表示我们要把用户 $u_i$ 买过的所有东西一个一个拿出来处理。
- $\alpha_i$
    - **定义**：**聚合权重（Aggregation Weight）**。
    - **含义**：它决定了“某次交互”对“用户画像”的贡献有多大。
    - **注意:** 这里的 $\alpha_i$ 通常被设定为 **固定值** $\frac{1}{|C(i)|}$ 。
        - 这意味着这是 **平均聚合** ：假设用户买过的所有东西同等重要
        - _后续进化_：论文在公式 (4) 中认为这样不够好，于是将其改进为 **Attention 权重** $\alpha_{ia}$，让模型自己学哪个物品更重要。
- $\mathbf{x}_{ia}$
    - **定义**：**Opinion-aware Interaction Representation（感知意见的交互表示）**。
    - **含义**：这是一个向量，它不仅仅是物品 $v_a$ 的 ID，而是 **“物品 $v_a$ 的向量”** 加上 **“用户打分 $r$ 的向量”** 经过 MLP 融合后的结果（即前文提到的公式 2）。它代表了那一次具体的交互事件。
- $\mathbf{b}$
    - **定义**：**偏置向量（Bias Vector）** 8。
    - **含义**：神经网络的标准配置，也是可训练的参数，用于调整激活函数的阈值位置。
- 这就相当于把所有历史记录取个平均值。
- **缺点：** 它假设你买的所有东西对你的兴趣定义同等重要。但实际上，你可能随手买了一包纸巾（不重要），和你买了一台高性能游戏本（很重要），这两者对你兴趣画像的贡献是不一样的 5。

**方案 B：注意力机制（Attention-based）—— 论文采用的方法** 

为了解决“平均法”的缺陷，作者引入了 Attention，让模型自己学每个交互的权重。

$$h_i^I = \sigma(W \cdot \{ \sum_{a \in C(i)} \alpha_{ia} x_{ia} \} + b)$$

- 这里 $\alpha_{ia}$ 就是**注意力权重（Attention Weight）**。它代表了物品 $v_a$ 的那次交互对定义用户 $u_i$ 有多重要。
### 权重 $\alpha_{ia}$ 怎么算？

作者用了一个双层神经网络（Attention Network）来计算。

$$x_{ia}^* = w_2^T \cdot \sigma(W_1 \cdot [x_{ia} \oplus p_i] + b_1) + b_2$$

$$\alpha_{ia} = \frac{\exp(x_{ia}^*)}{\sum_{a' \in C(i)} \exp(x_{ia'}^*)}$$

- **解读：**
- 输入：$[x_{ia} \oplus p_i]$。为什么要拼上 $p_i$（用户自身的基础 Embedding）？
	- 因为**同一个物品对不同用户的重要性不同**。必须把“当前物品交互 $x_{ia}$” 和 “目标用户 $p_i$” 放在一起看，才能算出这个物品对**这个**用户的重要程度。
 $W_1, w_2$：这是网络要学习的参数。
 Softmax：把算出来的分数 $x_{ia}^*$ 归一化，保证所有交互的权重加起来等于1。

## Social Aggregation（社交聚合）从朋友圈中刻画用户

### 聚合什么信息？

我们要把用户 $u_i$ 的朋友们（$o \in N(i)$）聚合起来。

注意： 这里聚合的不是朋友的基础 Embedding $p_o$，而是朋友的 Item-space 潜向量 $h_o^I$。
- **为什么？** 因为我们需要朋友的“特征”，而朋友的特征通过他自己的购买历史（$h_o^I$）体现得最淋漓尽致。
### 社交注意力机制（Social Attention）

同样，朋友的影响力有大有小（强关系 vs 弱关系），不能用平均值。所以也用了 Attention。

$$h_i^S = \sigma(W \cdot \{ \sum_{o \in N(i)} \beta_{io} h_o^I \} + b)$$

- $h_i^S$：用户 $u_i$ 的 **Social-space User Latent Factor**。
- $\beta_{io}$：朋友 $u_o$ 对用户 $u_i$ 的影响力权重。

$$\beta_{io}^* = w_2^T \cdot \sigma(W_1 \cdot [h_o^I \oplus p_i] + b_1) + b_2$$

- **解读：** 同样是把“朋友的特征 $h_o^I$”和“我自己的特征 $p_i$”拼起来，输入神经网络，计算出一个分数。这让模型能够模拟现实中的**强弱关系**。

## 合并

现在我们手里有了两块拼图：
1. $h_i^I$：基于行为历史的我。
2. $h_i^S$：基于社交关系的我。
把它们结合起来，得到最终的用户表示 $h_i$。

$$c_1 = [h_i^I \oplus h_i^S]$$

$$h_i = \text{MLP}(c_1)$$

(注：论文中是用多层神经网络表示的，公式13-14就是标准的MLP前向传播过程)
- **物理意义：** 这就是一个特征融合的过程。模型最终学到的 $h_i$ 既包含了用户的个人口味，也包含了社交圈对他的潜移默化影响 16。


# 物品建模

在 User Modeling 中，我们通过“我买过什么”和“我的朋友是谁”来定义“我是谁”。 在 **Item Modeling** 中，作者用了同样的逻辑：**一个物品的特征，是由“谁买过它”以及“用户对它的评价”来定义的。**

## 用户聚合
 交互表示 $f_{jt}$：

首先，我们要把每一个“用户-物品”的交互变成一个向量。

$$f_{jt} = g_u([p_t \oplus e_r])$$

- **$f_{jt}$**：这是 **Opinion-aware interaction user representation**。代表用户 $u_t$ 对物品 $v_j$ 的一次交互。

- **$p_t$**：用户 $u_t$ 的基础嵌入向量（Base Embedding）。

- **$e_r$**：意见嵌入（Opinion Embedding），比如“5星”的向量。

- **对比**：你会发现这跟 User Modeling 里的公式 (2) $x_{ia} = g_v([q_a \oplus e_r])$ 是一模一样的，只是把“物品 $q_a$”换成了“用户 $p_t$”。

## 计算物品特征

作者再次使用了 **注意力机制 (Attention Mechanism)**，因为不同用户对定义这个物品的重要性不同（资深影评人的评价可能比普通路人更重要）。
 聚合公式：
$$z_j = \sigma(W \cdot \{ \sum_{t \in B(j)} \mu_{jt} f_{jt} \} + b)$$
- **$z_j$**：物品 $v_j$ 的最终潜在因子（Latent Factor），也就是我们这一大步想求的结果。
- **$\mu_{jt}$**：**用户注意力权重**。代表用户 $u_t$ 对定义物品 $v_j$ 的重要程度。
权重计算：
$$\mu_{jt}^* = w_2^T \cdot \sigma(W_1 \cdot [f_{jt} \oplus q_j] + b_1) + b_2$$

$$\mu_{jt} = \frac{\exp(\mu_{jt}^*)}{\sum_{t' \in B(j)} \exp(\mu_{jt'}^*)}$$

- **输入**：$[f_{jt} \oplus q_j]$。把“交互信息”和“物品本身的基础向量 $q_j$”拼起来，通过两层神经网络算出权重。
小结 (Item Modeling)：
通过聚合所有消费者的信息 ($f_{jt}$) 并通过注意力加权 ($\mu_{jt}$)，我们得到了物品的高级特征向量 $z_j$。


# 预测公式

$$g_1 = [h_i \oplus z_j]$$

$$...$$

$$r'_{ij} = w^T \cdot g_{l-1}$$

- **逻辑**：非常简单粗暴。直接把 $h_i$ 和 $z_j$ **拼接 (Concatenate)** 起来，扔进一个多层感知机 (MLP)。

- **输出**：$r'_{ij}$，即预测的用户 $u_i$ 给物品 $v_j$ 的评分数值（比如预测出 4.3 分）。

# 模型训练

损失函数：
$$Loss = \frac{1}{2|O|} \sum_{i,j \in O} (r'_{ij} - r_{ij})^2$$
- **解释**：这就是最经典的 **均方误差 (MSE, Mean Squared Error)**。
    
    - $r'_{ij}$：模型预测的分数。
        
    - $r_{ij}$：真实的分数（Ground Truth）。
        
    - 目标：让预测值和真实值的差的平方越小越好。
        
- **优化器**：使用了 **RMSprop**，并使用了 **Dropout** 来防止过拟合。

# 实验部分

## 消融实验

“消融实验”——通过拆掉模型的某个零件，看看性能下降多少，来证明那个零件的重要性。

- **Q1: 社交关系真的有用吗？**
    
    - 对比：**GraphRec-SN**（去掉社交部分，只看历史行为）。
        
    - 结果：性能显著下降。
        
    - 结论：社交网络提供了互补的信息，很有用。
        
- **Q2: 意见（Opinion）嵌入有用吗？**
    
    - 对比：**GraphRec-Opinion**（去掉 $e_r$ 向量，只知道交互了，不知道是好评差评）。
        
    - 结果：性能下降得非常厉害。
        
    - 结论：把“给了几分”这个信息编码进去至关重要。
        
- **Q3: 注意力机制（Attention）有用吗？**
    
    - 对比：去掉 $\alpha$（物品注意力）或 $\beta$（社交注意力）。
        
    - 结果：性能都不如完整版。
        
    - 结论：区分朋友的亲疏、区分历史购买记录的重要性，是提升模型精度的关键。