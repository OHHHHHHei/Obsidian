# Introduction
## 传统推荐系统的依据

传统的推荐系统主要依据用户和物品的历史交互（比如你买了什么、点了什么赞）来推荐。但在现实中，我们的喜好往往会受到朋友的影响。

- **同质性 (Social Homophily):** 物以类聚，人以群分，朋友之间往往有相似的兴趣 。

- **社会影响 (Social Influence):** 你的行为会受到朋友行为的影响 。

因此，**社交推荐**就是把“用户-物品交互图”和“用户-社交关系图”结合起来，利用社交信息来辅助提高推荐准确率，特别是解决数据稀疏（用户买的东西很少）的问题 。

## 本文发现了什么痛点？(The Problem: Noise)

虽然引入社交网络是好事，但作者指出现在的社交网络中存在大量的**噪声 (Noise)** 。

- **通俗理解：** 微信里的好友未必都是和你志同道合的人。有些只是为了这就加上了（比如微商、为了点赞加的陌生人、或者仅仅是认识但兴趣完全不同的同学）。
    
- **数据证据：** 论文在 Figure 1 中展示了 Ciao 和 Yelp 两个数据集的统计，发现大部分有社交关系的用户之间，共同兴趣其实非常少 。
    
- **现有模型的缺陷：** 之前的模型（如 GNN 类模型）通常把社交关系图直接拿来用，这会导致“噪声”顺着图传播，反而干扰了模型对用户真实喜好的判断 。
    

## 这一篇论文的解决方案 (The Solution: DiffuSAR)

为了解决“社交噪声”问题，作者提出了 **DiffuSAR** 模型。

- **核心灵感：** 利用生成式 AI 中非常火的 **扩散模型 (Diffusion Model)**。扩散模型的强项就是“去噪”（比如把一张模糊的噪点图变成清晰的图）。
    
- **主要思路：**
    
    1. 把用户的社交特征看作是含噪的。
        
    2. 利用扩散模型生成一个**去噪后的、高质量的社交增强表示**。
        
    3. 设计一个**元网络 (Meta-network)**，把这个去噪后的社交知识“注入”到推荐系统中


# Social diffusion

## Forward Process

 $q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I)$
- **解释：** 这是一个马尔可夫链。$x_0$ 是原始的用户向量。$x_t$ 是第 $t$ 步加噪后的向量。
- $\alpha_t$ 是一个控制噪声大小的系数。
- $\mathcal{N}$ 代表高斯分布（正态分布）。意思是在上一步 $x_{t-1}$ 的基础上，加上一点点高斯噪声变成 $x_t$。

$q(x_t | x_0) = \mathcal{N}(x_t; \bar{\alpha}_t x_0, \sigma^2(t)I)$
- **解释：** 不需要一步步加噪，我们可以直接算出任意第 $t$ 步的噪声状态。
- $\bar{\alpha}(t)$：这是前面所有 $\alpha$ 的连乘积（通常写作 $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$）。
- $\sigma^2(t)$：这时的总噪声方差。
- **物理意义：** 随着 $t$ 变大（时间步增加），$x_t$ 会越来越接近纯噪声（标准正态分布），原始的用户信息 $x_0$ 逐渐消失。

## Reverse Process

我们要从纯噪声 $x_T$ 倒推回干净的信号 $x_0$。
### 马尔可夫链 (Markov Chain)
$$p_\theta(x_{0:T}) = p(x_T) \prod_{t=0}^T p_\theta(x_{t-1}|x_t), \quad p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t), \Sigma_\theta(x_t))$$

- 我们从 $x_T$（纯噪声）开始。
- $\prod$ 表示我们要一步步往回走：从 $T$ 推 $T-1$，再推 $T-2$... 直到。
- $p_\theta(x_{t-1}|x_t)$：这是一个由神经网络 $\theta$ 参数化的分布。它的任务是：**给你一个含噪的 $x_t$，请猜出它的上一步 $x_{t-1}$ 是什么**。
- **为什么要预测？**：正向加噪是确定的，但反向去噪是不确定的（因为加入的是随机噪声，我们不知道具体加了什么，所以要猜）。
- **预测什么？**：神经网络主要预测两个参数——**均值 $\mu_\theta(x_t)$** 和 **方差 $\Sigma_\theta(x_t)$** 。
### 均值的计算
$$\mu_\theta(x_t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \left[ \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \times \epsilon_\theta(x_t) \right] \right)$$

- **背景知识**：经过数学推导，我们发现**预测“上一步的图像”等价于预测“这一步加了什么噪声”**。
 A. 缩放系数：$\frac{1}{\sqrt{\alpha_t}}$
- **$\alpha_t$**：**噪声调度参数（Noise Schedule）**。这是一个预设的超参数（通常接近 1，比如 0.99...），控制每一步保留多少原始信息。
- **作用：** 这是一个归一化项。因为在正向加噪时，我们把数据乘以了 $\sqrt{\alpha_t}$（让信号变淡），所以在反向还原时，我们需要除以 $\sqrt{\alpha_t}$ 来把数据的幅度（Scale）拉回去。
当前状态：$x_t$
- **含义：** 当前第 $t$ 步的**含噪用户向量（Noisy User Embedding）**。
- **作用：** 这是我们的起点。我们要在这个“脏”数据的基准上进行修改（做减法）。
减去的噪声项：$\frac{1-\alpha_t}{\sqrt{1-\prod_{t'=0}^t \alpha_{t'}}} \epsilon_\theta(x_t)$
这是公式最复杂也最核心的部分，我们拆开看：
1. **$\epsilon_\theta(x_t)$ —— 神经网络的预测**
    - **$\epsilon$**：代表标准高斯噪声。
    - **$\theta$**：代表神经网络的参数（权重）。
    - **$\epsilon_\theta(x_t)$**：这是神经网络的**输出**。
    - **物理意义：** 神经网络看了一眼当前的 $x_t$，然后判断说：“我觉得这里面包含的**噪声**大概长这个样子”。
    - **关键点：** 扩散模型的神经网络**不是直接预测干净图像**，而是**预测加了什么噪声**。
2. **$\prod_{t'=0}^t \alpha_{t'}$ —— 累积噪声强度**
    - 这个符号通常记作 $\bar{\alpha}_t$（Alpha bar）。
    - 它代表从第 0 步到第 $t$ 步，$\alpha$ 连乘的结果。
    - 随着 $t$ 变大，这个连乘积会越来越接近 0，意味着原始信号几乎消失。
    - **$\sqrt{1-\prod \dots}$**：这一项其实代表了当前时刻 $t$，数据中总共包含的噪声的标准差。
3. **$\frac{1-\alpha_t}{\sqrt{\dots}}$ —— 噪声扣除系数**
    - 这是一个权重。它决定了我们要在 $x_t$ 中**扣除多少**神经网络预测出来的噪声。
- 直观理解公式 $$\text{上一时刻的数据} \approx \text{当前数据} - \text{神经网络预测出的噪声} \times \text{系数}$$
只要我们能训练一个好的神经网络 $\epsilon_\theta$，能准确预测出噪声，我们就能把噪声减掉，还原出信号。

## 条件引导

单纯的扩散模型生成的向量是随机的。我们需要生成的向量既像该用户，又包含其社交圈的特征。这就是**条件引导**的作用。
### 提取社交条件
$$e_u^c = \sum_{u' \in \mathcal{N}_u^{uu}} e'_{u'}$$
- **含义**：这是“条件嵌入 (Conditional Embedding)”的定义。
- **操作**：把用户 $u$ 在社交图 $G_{uu}$ 中所有邻居（$\mathcal{N}_u^{uu}$）的初始向量 $e'_{u'}$ 加起来。
- **物理意义**：$e_u^c$ 代表了“你的朋友们是谁”。如果你朋友都喜欢科技产品，这个 $e_u^c$ 就会包含强烈的科技属性特征。

### 分类器无关引导

$$\hat{\epsilon}_\theta(x_t | e_u^c) = \epsilon_\theta(x_t) + \lambda_s \cdot (\epsilon_\theta(x_t | e_u^c) - \epsilon_\theta(x_t))$$

- **背景**：我们需要训练神经网络预测噪声。公式展示了如何修正这个预测结果。
- **符号拆解**：
    - $\epsilon_\theta(x_t)$：**无条件预测**。神经网络只看噪声图 $x_t$，不看社交关系，凭空猜噪声。
    - $\epsilon_\theta(x_t | e_u^c)$：**有条件预测**。神经网络看着噪声图 $x_t$，同时参考社交关系 $e_u^c$，来猜噪声。
    - **$\lambda_s$ (Gradient Scale)**：这是一个**超参数**，用来控制“社交信号”有多强。
- **物理意义**：
    - $(\epsilon_\theta(x_t | e_u^c) - \epsilon_\theta(x_t))$ 代表了“社交信息带来的差异”。
    - 整个公式的意思是：最终的去噪方向 = 原始方向 + $\lambda_s$ 倍的（社交引导方向）。
    - **直观理解**：如果 $\lambda_s$ 很大，生成的向量就会极度接近朋友圈的特征；如果 $\lambda_s$ 很小，就更像用户原本的随机特征。这赋予了模型极好的灵活性。
## 神经网络内部实现

### 时间这一维怎么处理

扩散过程有 $T$ 步（比如 1000 步）。第 1 步（全是图）和第 999 步（全是噪点）的处理方式显然不同。神经网络需要知道“现在是几点”。

$$e_t = [\sin(\theta_j), \cos(\theta_j)]_{j=0}^{\frac{\hat{d}}{2}-1}, \quad \theta_j = t \cdot \frac{\log(10,000)}{\frac{\hat{d}}{2}-1} \cdot 2\pi \cdot j$$

- **含义**：这是**正弦位置编码 (Sinusoidal Position Embedding)**

### 网络结构 

![[Pasted image 20251122112849.png]]

- **不同于图像处理**：处理图像通常用 U-Net（卷积网络）。但推荐系统的 Embedding 是 1D 向量，所以作者改用了 **MLP (多层感知机)** 。
- **流程 (Fig 3)**：
    1. **输入**：当前含噪向量 $x_t + \epsilon$。
    2. **融合**：把时间嵌入 $t$、噪声向量、以及经过 Mask 处理的社交条件 $e_u^c$ 相加。
    3. **Condition Mask**：在训练时，随机把 $e_u^c$ 遮住（变成0）。为什么？为了强迫模型同时学会“有条件”和“无条件”的预测能力，这样才能用上面的公式。
    4. **处理**：通过 3 层全连接层 (FC * 3)。
    5. **输出**：预测的噪声 $\hat{\epsilon}$。
# 加速与优化

### 为什么要用 ODE？

传统的扩散模型反向采样需要几百上千步，推理速度太慢，不适合推荐系统。
作者引入了 DPM-Solver，这是一个基于常微分方程 (ODE) 的加速器。
$$\frac{d(x_t)}{dt} = f(t)x_t + \frac{g^2(t)}{2\sigma_t}\epsilon_\theta(x_t)$$

- **原理**：数学上可以证明，扩散过程等价于求解这个 ODE。
- **优势**：利用 ODE 求解器（类似龙格-库塔法那样的数值方法），可以用很大的步长跨越时间轴。作者说只需 **20 步** 就能完成采样，而不是原来的几百步。
### 离散化求解步骤 (顶部公式)

DPM-Solver 的具体迭代步骤：
$$\tilde{x}_{t_i} = \frac{\alpha_{t_i}}{\alpha_{t_{i-1}}} \tilde{x}_{t_{i-1}} - \sigma_{t_i}(e^{h_i}-1)\epsilon_\theta(\tilde{x}_{t_{i-1}})$$
- 这是对上述 ODE 的一阶或高阶离散化解法 11。
- $h_i$ 是时间步长。
- 这一步算完，我们就得到了最终的去噪向量 $e_u^{diffu}$ 12。
### 损失函数 (Diffusion Loss)

模型怎么训练呢？
$$\mathcal{L}_{DIM_\theta} = \mathbb{E}_{t, x_0, \epsilon} [ ||\epsilon - \hat{\epsilon}_\theta(x_t | e_u^c)||^2 ]$$
- **目标**：最小化预测误差。
- **解释**：
    - $\epsilon$：这是**真实**添加的噪声（我们在正向过程中自己加的，所以是已知的 Answer）。
    - $\hat{\epsilon}_\theta$：这是神经网络**预测**的噪声。
    - **Mean Squared Error (MSE)**：算它俩的欧氏距离（范数）。